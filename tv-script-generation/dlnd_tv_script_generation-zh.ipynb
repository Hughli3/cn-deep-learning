{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成电视剧剧本\n",
    "\n",
    "在这个项目中，你将使用 RNN 创作你自己的[《辛普森一家》](https://zh.wikipedia.org/wiki/%E8%BE%9B%E6%99%AE%E6%A3%AE%E4%B8%80%E5%AE%B6)电视剧剧本。你将会用到《辛普森一家》第 27 季中部分剧本的[数据集](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data)。你创建的神经网络将为一个在 [Moe 酒馆](https://simpsonswiki.com/wiki/Moe's_Tavern)中的场景生成一集新的剧本。\n",
    "## 获取数据\n",
    "我们早已为你提供了数据。你将使用原始数据集的子集，它只包括 Moe 酒馆中的场景。数据中并不包括酒馆的其他版本，比如 “Moe 的山洞”、“燃烧的 Moe 酒馆”、“Moe 叔叔的家庭大餐”等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "\n",
    "data_dir = './data/simpsons/moes_tavern_lines.txt'\n",
    "text = helper.load_data(data_dir)\n",
    "# Ignore notice, since we don't use it for analysing the data\n",
    "text = text[81:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探索数据\n",
    "使用 `view_sentence_range` 来查看数据的不同部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 11492\n",
      "Number of scenes: 262\n",
      "Average number of sentences in each scene: 15.248091603053435\n",
      "Number of lines: 4257\n",
      "Average number of words in each line: 11.50434578341555\n",
      "\n",
      "The sentences 0 to 10:\n",
      "Moe_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.\n",
      "Bart_Simpson: Eh, yeah, hello, is Mike there? Last name, Rotch.\n",
      "Moe_Szyslak: (INTO PHONE) Hold on, I'll check. (TO BARFLIES) Mike Rotch. Mike Rotch. Hey, has anybody seen Mike Rotch, lately?\n",
      "Moe_Szyslak: (INTO PHONE) Listen you little puke. One of these days I'm gonna catch you, and I'm gonna carve my name on your back with an ice pick.\n",
      "Moe_Szyslak: What's the matter Homer? You're not your normal effervescent self.\n",
      "Homer_Simpson: I got my problems, Moe. Give me another one.\n",
      "Moe_Szyslak: Homer, hey, you should not drink to forget your problems.\n",
      "Barney_Gumble: Yeah, you should only drink to enhance your social skills.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "scenes = text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现预处理函数\n",
    "对数据集进行的第一个操作是预处理。请实现下面两个预处理函数：\n",
    "\n",
    "- 查询表\n",
    "- 标记符号的字符串\n",
    "\n",
    "### 查询表\n",
    "要创建词嵌入，你首先要将词语转换为 id。请在这个函数中创建两个字典：\n",
    "\n",
    "- 将词语转换为 id 的字典，我们称它为 `vocab_to_int`\n",
    "- 将 id 转换为词语的字典，我们称它为 `int_to_vocab`\n",
    "\n",
    "请在下面的元组中返回这些字典\n",
    " `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "from collections import Counter\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    counts = Counter(text)\n",
    "    sorted_vocab = sorted(counts, key = counts.get , reverse = True)\n",
    "    vocab_to_int = {word: ii for ii , word in enumerate(sorted_vocab, 0)}\n",
    "    int_to_vocab = {number: ii for ii, number in vocab_to_int.items()}\n",
    "    return vocab_to_int, int_to_vocab\n",
    "# dict = {'Google': 'www.google.com', 'Runoob': 'www.runoob.com', 'taobao': 'www.taobao.com'}\n",
    "# 遍历字典列表\n",
    "# for key,values in  dict.items():\n",
    "#     print key,values\n",
    "# 输出是\n",
    "# Google www.google.com\n",
    "# taobao www.taobao.com\n",
    "# Runoob www.runoob.com\n",
    "# 注意：遍历item()的系统的时候要key,values两个一起遍历单个不行\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '||period||',\n",
       " 1: '||return||',\n",
       " 2: '||comma||',\n",
       " 3: '||left_parenthesis||',\n",
       " 4: '||right_parenthesis||',\n",
       " 5: 'the',\n",
       " 6: 'i',\n",
       " 7: 'you',\n",
       " 8: '||exclamation_mark||',\n",
       " 9: 'moe_szyslak:',\n",
       " 10: '||question_mark||',\n",
       " 11: 'a',\n",
       " 12: 'homer_simpson:',\n",
       " 13: 'to',\n",
       " 14: 'and',\n",
       " 15: 'of',\n",
       " 16: 'my',\n",
       " 17: 'it',\n",
       " 18: 'that',\n",
       " 19: 'in',\n",
       " 20: '||quotation_mark||',\n",
       " 21: 'me',\n",
       " 22: 'is',\n",
       " 23: 'this',\n",
       " 24: \"i'm\",\n",
       " 25: 'for',\n",
       " 26: 'your',\n",
       " 27: 'homer',\n",
       " 28: 'hey',\n",
       " 29: 'on',\n",
       " 30: 'moe',\n",
       " 31: 'oh',\n",
       " 32: 'no',\n",
       " 33: 'lenny_leonard:',\n",
       " 34: 'what',\n",
       " 35: 'with',\n",
       " 36: 'yeah',\n",
       " 37: 'all',\n",
       " 38: 'just',\n",
       " 39: 'like',\n",
       " 40: 'but',\n",
       " 41: 'barney_gumble:',\n",
       " 42: 'so',\n",
       " 43: 'be',\n",
       " 44: 'here',\n",
       " 45: 'carl_carlson:',\n",
       " 46: \"don't\",\n",
       " 47: 'have',\n",
       " 48: 'up',\n",
       " 49: \"it's\",\n",
       " 50: 'well',\n",
       " 51: 'out',\n",
       " 52: 'do',\n",
       " 53: 'was',\n",
       " 54: 'got',\n",
       " 55: 'get',\n",
       " 56: 'are',\n",
       " 57: 'we',\n",
       " 58: \"that's\",\n",
       " 59: 'uh',\n",
       " 60: 'one',\n",
       " 61: \"you're\",\n",
       " 62: 'now',\n",
       " 63: 'not',\n",
       " 64: 'know',\n",
       " 65: 'can',\n",
       " 66: '||dash||',\n",
       " 67: 'at',\n",
       " 68: 'right',\n",
       " 69: '/',\n",
       " 70: 'how',\n",
       " 71: 'if',\n",
       " 72: 'back',\n",
       " 73: 'marge_simpson:',\n",
       " 74: 'about',\n",
       " 75: 'he',\n",
       " 76: 'from',\n",
       " 77: 'go',\n",
       " 78: 'gonna',\n",
       " 79: 'they',\n",
       " 80: 'there',\n",
       " 81: 'beer',\n",
       " 82: 'good',\n",
       " 83: 'who',\n",
       " 84: 'an',\n",
       " 85: 'man',\n",
       " 86: 'okay',\n",
       " 87: 'his',\n",
       " 88: 'little',\n",
       " 89: 'as',\n",
       " 90: 'some',\n",
       " 91: \"can't\",\n",
       " 92: 'then',\n",
       " 93: 'never',\n",
       " 94: 'come',\n",
       " 95: \"i'll\",\n",
       " 96: 'think',\n",
       " 97: 'could',\n",
       " 98: 'him',\n",
       " 99: \"i've\",\n",
       " 100: 'look',\n",
       " 101: 'really',\n",
       " 102: 'want',\n",
       " 103: 'see',\n",
       " 104: 'too',\n",
       " 105: 'been',\n",
       " 106: 'guys',\n",
       " 107: 'when',\n",
       " 108: 'make',\n",
       " 109: 'why',\n",
       " 110: 'bar',\n",
       " 111: 'ya',\n",
       " 112: 'her',\n",
       " 113: 'did',\n",
       " 114: 'say',\n",
       " 115: 'time',\n",
       " 116: 'gotta',\n",
       " 117: 'marge',\n",
       " 118: 'or',\n",
       " 119: 'ah',\n",
       " 120: 'take',\n",
       " 121: 'into',\n",
       " 122: 'love',\n",
       " 123: 'down',\n",
       " 124: 'more',\n",
       " 125: 'our',\n",
       " 126: 'am',\n",
       " 127: 'off',\n",
       " 128: 'guy',\n",
       " 129: 'sure',\n",
       " 130: 'two',\n",
       " 131: 'barney',\n",
       " 132: \"there's\",\n",
       " 133: 'thing',\n",
       " 134: 'lisa_simpson:',\n",
       " 135: 'would',\n",
       " 136: \"we're\",\n",
       " 137: 'big',\n",
       " 138: 'where',\n",
       " 139: 'need',\n",
       " 140: 'let',\n",
       " 141: \"he's\",\n",
       " 142: 'tell',\n",
       " 143: 'had',\n",
       " 144: 'money',\n",
       " 145: \"what's\",\n",
       " 146: 'drink',\n",
       " 147: 'sorry',\n",
       " 148: 'something',\n",
       " 149: 'over',\n",
       " 150: 'us',\n",
       " 151: 'bart_simpson:',\n",
       " 152: 'ever',\n",
       " 153: 'by',\n",
       " 154: 'only',\n",
       " 155: 'day',\n",
       " 156: 'will',\n",
       " 157: 'way',\n",
       " 158: 'wait',\n",
       " 159: 'chief_wiggum:',\n",
       " 160: 'she',\n",
       " 161: 'give',\n",
       " 162: 'even',\n",
       " 163: 'god',\n",
       " 164: \"i'd\",\n",
       " 165: 'huh',\n",
       " 166: 'new',\n",
       " 167: \"didn't\",\n",
       " 168: \"ain't\",\n",
       " 169: 'those',\n",
       " 170: 'people',\n",
       " 171: 'great',\n",
       " 172: \"moe's\",\n",
       " 173: 'maybe',\n",
       " 174: 'lenny',\n",
       " 175: 'has',\n",
       " 176: 'life',\n",
       " 177: 'phone',\n",
       " 178: 'were',\n",
       " 179: 'eh',\n",
       " 180: 'much',\n",
       " 181: 'than',\n",
       " 182: 'mean',\n",
       " 183: 'going',\n",
       " 184: 'place',\n",
       " 185: 'should',\n",
       " 186: \"you've\",\n",
       " 187: 'around',\n",
       " 188: 'wanna',\n",
       " 189: 'mr',\n",
       " 190: 'still',\n",
       " 191: 'these',\n",
       " 192: 'better',\n",
       " 193: \"'em\",\n",
       " 194: 'friend',\n",
       " 195: 'home',\n",
       " 196: 'help',\n",
       " 197: 'old',\n",
       " 198: 'please',\n",
       " 199: 'night',\n",
       " 200: 'before',\n",
       " 201: 'name',\n",
       " 202: 'noise',\n",
       " 203: 'aw',\n",
       " 204: 'seymour_skinner:',\n",
       " 205: 'last',\n",
       " 206: 'whoa',\n",
       " 207: 'tv',\n",
       " 208: 'face',\n",
       " 209: 'any',\n",
       " 210: 'boy',\n",
       " 211: 'made',\n",
       " 212: 'three',\n",
       " 213: 'duff',\n",
       " 214: 'call',\n",
       " 215: 'thanks',\n",
       " 216: 'put',\n",
       " 217: 'drunk',\n",
       " 218: 'hello',\n",
       " 219: \"'cause\",\n",
       " 220: 'looking',\n",
       " 221: 'listen',\n",
       " 222: 'their',\n",
       " 223: 'bad',\n",
       " 224: 'car',\n",
       " 225: 'again',\n",
       " 226: 'very',\n",
       " 227: \"let's\",\n",
       " 228: 'best',\n",
       " 229: 'first',\n",
       " 230: 'does',\n",
       " 231: 'wow',\n",
       " 232: 'yes',\n",
       " 233: 'ooh',\n",
       " 234: 'kent_brockman:',\n",
       " 235: 'every',\n",
       " 236: 'them',\n",
       " 237: 'said',\n",
       " 238: 'another',\n",
       " 239: 'looks',\n",
       " 240: 'while',\n",
       " 241: 'other',\n",
       " 242: 'wife',\n",
       " 243: 'guess',\n",
       " 244: 'apu_nahasapeemapetilon:',\n",
       " 245: 'work',\n",
       " 246: 'singing',\n",
       " 247: 'feel',\n",
       " 248: \"won't\",\n",
       " 249: 'play',\n",
       " 250: 'sweet',\n",
       " 251: 'years',\n",
       " 252: 'dad',\n",
       " 253: 'tonight',\n",
       " 254: 'springfield',\n",
       " 255: \"they're\",\n",
       " 256: 'after',\n",
       " 257: 'dr',\n",
       " 258: 'voice',\n",
       " 259: 'find',\n",
       " 260: 'everybody',\n",
       " 261: 'thought',\n",
       " 262: 'sobs',\n",
       " 263: 'kids',\n",
       " 264: 'things',\n",
       " 265: 'buy',\n",
       " 266: 'might',\n",
       " 267: 'world',\n",
       " 268: 'show',\n",
       " 269: 'beat',\n",
       " 270: 'because',\n",
       " 271: 'keep',\n",
       " 272: 'girl',\n",
       " 273: 'nice',\n",
       " 274: 'head',\n",
       " 275: 'check',\n",
       " 276: 'since',\n",
       " 277: 'happy',\n",
       " 278: 'minute',\n",
       " 279: 'shut',\n",
       " 280: 'lisa',\n",
       " 281: 'friends',\n",
       " 282: 'sighs',\n",
       " 283: \"who's\",\n",
       " 284: 'always',\n",
       " 285: 'bart',\n",
       " 286: 'sings',\n",
       " 287: 'use',\n",
       " 288: 'chuckle',\n",
       " 289: 'stupid',\n",
       " 290: 'kid',\n",
       " 291: 'which',\n",
       " 292: \"you'll\",\n",
       " 293: 'someone',\n",
       " 294: 'c',\n",
       " 295: 'krusty_the_clown:',\n",
       " 296: 'carl',\n",
       " 297: \"isn't\",\n",
       " 298: 'ow',\n",
       " 299: \"here's\",\n",
       " 300: 'lot',\n",
       " 301: 'seen',\n",
       " 302: 'talk',\n",
       " 303: 'remember',\n",
       " 304: 'anything',\n",
       " 305: 'hundred',\n",
       " 306: 'laugh',\n",
       " 307: 'chuckles',\n",
       " 308: 'glass',\n",
       " 309: 'through',\n",
       " 310: 'job',\n",
       " 311: 'thank',\n",
       " 312: 'laughs',\n",
       " 313: 'next',\n",
       " 314: 'hell',\n",
       " 315: 'simpson',\n",
       " 316: 'says',\n",
       " 317: 'matter',\n",
       " 318: 'lost',\n",
       " 319: 'happened',\n",
       " 320: 'believe',\n",
       " 321: 'away',\n",
       " 322: 'hear',\n",
       " 323: 'long',\n",
       " 324: 'five',\n",
       " 325: 'woman',\n",
       " 326: 'pretty',\n",
       " 327: \"nothin'\",\n",
       " 328: 'hope',\n",
       " 329: 'outta',\n",
       " 330: 'kind',\n",
       " 331: 'house',\n",
       " 332: 'book',\n",
       " 333: 'comes',\n",
       " 334: 'tavern',\n",
       " 335: 'nervous',\n",
       " 336: 'real',\n",
       " 337: 'wish',\n",
       " 338: '_montgomery_burns:',\n",
       " 339: 'family',\n",
       " 340: 'turn',\n",
       " 341: 'waylon_smithers:',\n",
       " 342: 'fat',\n",
       " 343: \"c'mon\",\n",
       " 344: 'once',\n",
       " 345: 'ned_flanders:',\n",
       " 346: 'stop',\n",
       " 347: \"homer's\",\n",
       " 348: 'four',\n",
       " 349: 'game',\n",
       " 350: 'idea',\n",
       " 351: \"goin'\",\n",
       " 352: 'myself',\n",
       " 353: 'ask',\n",
       " 354: 'business',\n",
       " 355: 'wants',\n",
       " 356: 'wrong',\n",
       " 357: 'grampa_simpson:',\n",
       " 358: \"doin'\",\n",
       " 359: 'actually',\n",
       " 360: \"we've\",\n",
       " 361: 'today',\n",
       " 362: 'everything',\n",
       " 363: 'nobody',\n",
       " 364: \"wouldn't\",\n",
       " 365: 'getting',\n",
       " 366: \"she's\",\n",
       " 367: 'used',\n",
       " 368: 'must',\n",
       " 369: 'problem',\n",
       " 370: \"comin'\",\n",
       " 371: 'enough',\n",
       " 372: 'duffman:',\n",
       " 373: 'burns',\n",
       " 374: 'loud',\n",
       " 375: 'done',\n",
       " 376: 'town',\n",
       " 377: 'party',\n",
       " 378: 'many',\n",
       " 379: 'bucks',\n",
       " 380: 'maggie',\n",
       " 381: 'thinking',\n",
       " 382: 'woo',\n",
       " 383: 'um',\n",
       " 384: 'took',\n",
       " 385: 'dollars',\n",
       " 386: 'sound',\n",
       " 387: 'true',\n",
       " 388: 'free',\n",
       " 389: 'doing',\n",
       " 390: 'hold',\n",
       " 391: 'try',\n",
       " 392: 'watch',\n",
       " 393: 'reading',\n",
       " 394: 'na',\n",
       " 395: 'sounds',\n",
       " 396: 'nah',\n",
       " 397: 'gee',\n",
       " 398: 'leave',\n",
       " 399: 'yourself',\n",
       " 400: 'beautiful',\n",
       " 401: 'makes',\n",
       " 402: 'edna',\n",
       " 403: 'gimme',\n",
       " 404: 'most',\n",
       " 405: \"where's\",\n",
       " 406: 'stuff',\n",
       " 407: 'sell',\n",
       " 408: 'pay',\n",
       " 409: 'being',\n",
       " 410: 'pants',\n",
       " 411: 'wanted',\n",
       " 412: 'under',\n",
       " 413: 'chief',\n",
       " 414: 'baby',\n",
       " 415: 'excuse',\n",
       " 416: 'care',\n",
       " 417: 'everyone',\n",
       " 418: 'secret',\n",
       " 419: 'daughter',\n",
       " 420: 'kill',\n",
       " 421: 'canyonero',\n",
       " 422: 'kemi:',\n",
       " 423: 'pick',\n",
       " 424: 'smithers',\n",
       " 425: 'quickly',\n",
       " 426: \"you'd\",\n",
       " 427: 'left',\n",
       " 428: 'knew',\n",
       " 429: 'tipsy',\n",
       " 430: 'hurt',\n",
       " 431: 'mouth',\n",
       " 432: 'went',\n",
       " 433: 'tough',\n",
       " 434: 'dead',\n",
       " 435: 'pal',\n",
       " 436: 'save',\n",
       " 437: 'own',\n",
       " 438: 'sad',\n",
       " 439: 'dinner',\n",
       " 440: 'points',\n",
       " 441: 'worry',\n",
       " 442: 'ladies',\n",
       " 443: 'drinking',\n",
       " 444: 'hate',\n",
       " 445: 'feeling',\n",
       " 446: 'hoo',\n",
       " 447: 'saw',\n",
       " 448: 'break',\n",
       " 449: 'hi',\n",
       " 450: 'booze',\n",
       " 451: 'till',\n",
       " 452: 'skinner',\n",
       " 453: 'gets',\n",
       " 454: 'told',\n",
       " 455: 'die',\n",
       " 456: 'win',\n",
       " 457: 'came',\n",
       " 458: 'dog',\n",
       " 459: 'funny',\n",
       " 460: 'quit',\n",
       " 461: 'tomorrow',\n",
       " 462: 'heard',\n",
       " 463: 'fine',\n",
       " 464: 'excited',\n",
       " 465: 'eyes',\n",
       " 466: 'camera',\n",
       " 467: 'school',\n",
       " 468: 'sign',\n",
       " 469: 'gave',\n",
       " 470: 'read',\n",
       " 471: 'date',\n",
       " 472: 'clean',\n",
       " 473: 'hand',\n",
       " 474: 'loser',\n",
       " 475: 'million',\n",
       " 476: 'jacques:',\n",
       " 477: 'kinda',\n",
       " 478: 'easy',\n",
       " 479: 'forget',\n",
       " 480: 'surprised',\n",
       " 481: 'nuts',\n",
       " 482: \"aren't\",\n",
       " 483: 'artie_ziff:',\n",
       " 484: 'super',\n",
       " 485: 'drive',\n",
       " 486: 'mad',\n",
       " 487: 'gasp',\n",
       " 488: 'krusty',\n",
       " 489: 'alcohol',\n",
       " 490: 'room',\n",
       " 491: 'calling',\n",
       " 492: 'cash',\n",
       " 493: 'mom',\n",
       " 494: 'bring',\n",
       " 495: 'anyone',\n",
       " 496: 'loves',\n",
       " 497: 'barflies:',\n",
       " 498: 'sir',\n",
       " 499: \"couldn't\",\n",
       " 500: 'without',\n",
       " 501: 'small',\n",
       " 502: 'seven',\n",
       " 503: 'fire',\n",
       " 504: 'fight',\n",
       " 505: 'kirk_van_houten:',\n",
       " 506: 'gone',\n",
       " 507: 'eat',\n",
       " 508: 'hands',\n",
       " 509: 'twenty',\n",
       " 510: 'flaming',\n",
       " 511: 'burn',\n",
       " 512: 'noises',\n",
       " 513: 'course',\n",
       " 514: \"y'know\",\n",
       " 515: 'trouble',\n",
       " 516: 'turns',\n",
       " 517: 'coming',\n",
       " 518: 'tape',\n",
       " 519: 'happen',\n",
       " 520: 'cut',\n",
       " 521: 'chance',\n",
       " 522: 'gentlemen',\n",
       " 523: 'seymour',\n",
       " 524: \"wasn't\",\n",
       " 525: \"somethin'\",\n",
       " 526: 'sadly',\n",
       " 527: \"lookin'\",\n",
       " 528: 'meet',\n",
       " 529: 'door',\n",
       " 530: 'bartender',\n",
       " 531: 'each',\n",
       " 532: 'six',\n",
       " 533: 'professor_jonathan_frink:',\n",
       " 534: 'anyway',\n",
       " 535: \"haven't\",\n",
       " 536: \"we'll\",\n",
       " 537: 'geez',\n",
       " 538: \"talkin'\",\n",
       " 539: 'dear',\n",
       " 540: 'called',\n",
       " 541: \"doesn't\",\n",
       " 542: 'high',\n",
       " 543: 'low',\n",
       " 544: 'upset',\n",
       " 545: \"drinkin'\",\n",
       " 546: 'behind',\n",
       " 547: 'already',\n",
       " 548: 'problems',\n",
       " 549: 'hours',\n",
       " 550: \"it'll\",\n",
       " 551: 'worse',\n",
       " 552: 'song',\n",
       " 553: 'eye',\n",
       " 554: 'straight',\n",
       " 555: 'lady',\n",
       " 556: 'talking',\n",
       " 557: 'live',\n",
       " 558: 'steal',\n",
       " 559: 'crazy',\n",
       " 560: 'ugly',\n",
       " 561: 'worried',\n",
       " 562: 'moan',\n",
       " 563: 'end',\n",
       " 564: 'm',\n",
       " 565: 'works',\n",
       " 566: 'sigh',\n",
       " 567: 'heart',\n",
       " 568: 'whole',\n",
       " 569: 'bar_rag:',\n",
       " 570: 'stand',\n",
       " 571: 'sing',\n",
       " 572: 'hot',\n",
       " 573: 'bottle',\n",
       " 574: 'stay',\n",
       " 575: 'wiggum',\n",
       " 576: 'snake_jailbird:',\n",
       " 577: 'learn',\n",
       " 578: 'keys',\n",
       " 579: 'dump',\n",
       " 580: 'mmmm',\n",
       " 581: 'war',\n",
       " 582: 'spend',\n",
       " 583: 'self',\n",
       " 584: 'watching',\n",
       " 585: 'the_rich_texan:',\n",
       " 586: 'outside',\n",
       " 587: 'uh-oh',\n",
       " 588: 'machine',\n",
       " 589: 'disgusted',\n",
       " 590: 'fun',\n",
       " 591: 'change',\n",
       " 592: 'whatever',\n",
       " 593: 'throat',\n",
       " 594: 'least',\n",
       " 595: 'start',\n",
       " 596: ':',\n",
       " 597: 'realizing',\n",
       " 598: 'larry:',\n",
       " 599: 'quiet',\n",
       " 600: 'trying',\n",
       " 601: 'blame',\n",
       " 602: \"gettin'\",\n",
       " 603: 'close',\n",
       " 604: 'poor',\n",
       " 605: 'such',\n",
       " 606: 'girls',\n",
       " 607: 'gasps',\n",
       " 608: 'crowd:',\n",
       " 609: 'anymore',\n",
       " 610: 'private',\n",
       " 611: 'either',\n",
       " 612: 'annoyed',\n",
       " 613: 'apu',\n",
       " 614: 'late',\n",
       " 615: 'morning',\n",
       " 616: 'soon',\n",
       " 617: 'stick',\n",
       " 618: 'barflies',\n",
       " 619: 'less',\n",
       " 620: 'else',\n",
       " 621: 'probably',\n",
       " 622: 'bit',\n",
       " 623: 'butt',\n",
       " 624: 'marriage',\n",
       " 625: 'playing',\n",
       " 626: 'special',\n",
       " 627: 'ten',\n",
       " 628: 'alive',\n",
       " 629: 'thinks',\n",
       " 630: 'pull',\n",
       " 631: 'tsk',\n",
       " 632: 'delete',\n",
       " 633: 'perfect',\n",
       " 634: 'police',\n",
       " 635: 'thousand',\n",
       " 636: 'greatest',\n",
       " 637: 'second',\n",
       " 638: 'point',\n",
       " 639: 'whip',\n",
       " 640: 'times',\n",
       " 641: \"shouldn't\",\n",
       " 642: 'joe',\n",
       " 643: 'person',\n",
       " 644: 'blue',\n",
       " 645: 'light',\n",
       " 646: 'year',\n",
       " 647: 'air',\n",
       " 648: 'drinks',\n",
       " 649: 'mother',\n",
       " 650: 'heh',\n",
       " 651: 'bowl',\n",
       " 652: 'front',\n",
       " 653: 'learned',\n",
       " 654: 'far',\n",
       " 655: 'lousy',\n",
       " 656: 'shocked',\n",
       " 657: 'ma',\n",
       " 658: 'goodbye',\n",
       " 659: 'married',\n",
       " 660: \"'bout\",\n",
       " 661: 'may',\n",
       " 662: 'young',\n",
       " 663: 'turned',\n",
       " 664: 'join',\n",
       " 665: 'shot',\n",
       " 666: 'knows',\n",
       " 667: 'boxing',\n",
       " 668: 'taking',\n",
       " 669: 'smell',\n",
       " 670: 'anybody',\n",
       " 671: 'goodnight',\n",
       " 672: 'jacques',\n",
       " 673: 'lemme',\n",
       " 674: 'goes',\n",
       " 675: 'christmas',\n",
       " 676: 'later',\n",
       " 677: 'miss',\n",
       " 678: 'kick',\n",
       " 679: 'agnes_skinner:',\n",
       " 680: 'serious',\n",
       " 681: 'patty_bouvier:',\n",
       " 682: 'angry',\n",
       " 683: 'fifty',\n",
       " 684: 'nothing',\n",
       " 685: 'mayor_joe_quimby:',\n",
       " 686: 'lucky',\n",
       " 687: 'both',\n",
       " 688: 'blood',\n",
       " 689: \"how'd\",\n",
       " 690: 'president',\n",
       " 691: 'picture',\n",
       " 692: 'store',\n",
       " 693: 'eight',\n",
       " 694: 'nods',\n",
       " 695: 'card',\n",
       " 696: 'throw',\n",
       " 697: 'shotgun',\n",
       " 698: 'cool',\n",
       " 699: 'pig',\n",
       " 700: 'youse',\n",
       " 701: 'couple',\n",
       " 702: 'found',\n",
       " 703: 'boys',\n",
       " 704: 'walk',\n",
       " 705: 'ticket',\n",
       " 706: 'duffman',\n",
       " 707: 'king',\n",
       " 708: 'open',\n",
       " 709: 'ass',\n",
       " 710: 'alone',\n",
       " 711: 'letter',\n",
       " 712: 'buddy',\n",
       " 713: 'tab',\n",
       " 714: 'uh-huh',\n",
       " 715: 'feet',\n",
       " 716: 'moron',\n",
       " 717: 'mind',\n",
       " 718: 'turning',\n",
       " 719: 'rev',\n",
       " 720: 'exactly',\n",
       " 721: 'american',\n",
       " 722: 'box',\n",
       " 723: 'barn',\n",
       " 724: 'instead',\n",
       " 725: 'beers',\n",
       " 726: 'yet',\n",
       " 727: 'arm',\n",
       " 728: 'minutes',\n",
       " 729: 'run',\n",
       " 730: '||semicolon||',\n",
       " 731: 'welcome',\n",
       " 732: 'happier',\n",
       " 733: 'narrator:',\n",
       " 734: 'table',\n",
       " 735: 'sotto',\n",
       " 736: 'sitting',\n",
       " 737: 'using',\n",
       " 738: 'lucius:',\n",
       " 739: 'hang',\n",
       " 740: 'ahh',\n",
       " 741: 'send',\n",
       " 742: 'seat',\n",
       " 743: 'renee:',\n",
       " 744: 'inside',\n",
       " 745: 'rummy',\n",
       " 746: 'alright',\n",
       " 747: 'ready',\n",
       " 748: 'top',\n",
       " 749: 'street',\n",
       " 750: 'horrible',\n",
       " 751: 'o',\n",
       " 752: 'intrigued',\n",
       " 753: 'advice',\n",
       " 754: 'collette:',\n",
       " 755: 'cold',\n",
       " 756: 'scared',\n",
       " 757: 'paint',\n",
       " 758: 'grampa',\n",
       " 759: 'full',\n",
       " 760: 'himself',\n",
       " 761: 'homie',\n",
       " 762: 'accident',\n",
       " 763: 'move',\n",
       " 764: 'losers',\n",
       " 765: 'glad',\n",
       " 766: 'screw',\n",
       " 767: 'son',\n",
       " 768: 'news',\n",
       " 769: 'amazed',\n",
       " 770: 'somebody',\n",
       " 771: 'saying',\n",
       " 772: 'telling',\n",
       " 773: 'young_marge:',\n",
       " 774: 'smile',\n",
       " 775: 'channel',\n",
       " 776: 'proudly',\n",
       " 777: 'la',\n",
       " 778: 'round',\n",
       " 779: \"he'll\",\n",
       " 780: 'kiss',\n",
       " 781: 'food',\n",
       " 782: 'butts',\n",
       " 783: 'tap',\n",
       " 784: 'warm_female_voice:',\n",
       " 785: 'plant',\n",
       " 786: 'black',\n",
       " 787: 'glove',\n",
       " 788: 'though',\n",
       " 789: 'friendly',\n",
       " 790: 'means',\n",
       " 791: 'eggs',\n",
       " 792: 'selma_bouvier:',\n",
       " 793: 'weird',\n",
       " 794: 'fast',\n",
       " 795: 'walking',\n",
       " 796: \"ol'\",\n",
       " 797: 'warmly',\n",
       " 798: 'huge',\n",
       " 799: 'crap',\n",
       " 800: 'story',\n",
       " 801: \"kiddin'\",\n",
       " 802: 'favorite',\n",
       " 803: 'etc',\n",
       " 804: 'human',\n",
       " 805: 'forever',\n",
       " 806: 'together',\n",
       " 807: 'deal',\n",
       " 808: 'making',\n",
       " 809: 'hide',\n",
       " 810: 'japanese',\n",
       " 811: 'english',\n",
       " 812: 'terrible',\n",
       " 813: 'peanuts',\n",
       " 814: \"makin'\",\n",
       " 815: 'ned',\n",
       " 816: 'mrs',\n",
       " 817: 'seems',\n",
       " 818: 'ha',\n",
       " 819: 'fellas',\n",
       " 820: 'pour',\n",
       " 821: 'days',\n",
       " 822: 'plus',\n",
       " 823: 'nine',\n",
       " 824: 'dance',\n",
       " 825: 'hmm',\n",
       " 826: 'return',\n",
       " 827: 'words',\n",
       " 828: 'number',\n",
       " 829: 'word',\n",
       " 830: 'city',\n",
       " 831: 's',\n",
       " 832: 'moans',\n",
       " 833: 'damn',\n",
       " 834: 'finally',\n",
       " 835: 'broke',\n",
       " 836: 'comic_book_guy:',\n",
       " 837: 'dry',\n",
       " 838: 'wire',\n",
       " 839: 'afraid',\n",
       " 840: 'ball',\n",
       " 841: 'supposed',\n",
       " 842: 'normal',\n",
       " 843: 'princess',\n",
       " 844: 'write',\n",
       " 845: 'fat_tony:',\n",
       " 846: 'invented',\n",
       " 847: 'brought',\n",
       " 848: 'giving',\n",
       " 849: 'fill',\n",
       " 850: 'bow',\n",
       " 851: 'state',\n",
       " 852: 'lowers',\n",
       " 853: 'manjula_nahasapeemapetilon:',\n",
       " 854: 'crack',\n",
       " 855: 'young_homer:',\n",
       " 856: 'forgot',\n",
       " 857: \"sayin'\",\n",
       " 858: 'lying',\n",
       " 859: 'baseball',\n",
       " 860: 'principal',\n",
       " 861: 'eating',\n",
       " 862: 'omigod',\n",
       " 863: 'honest',\n",
       " 864: 'touched',\n",
       " 865: 'half',\n",
       " 866: 'strong',\n",
       " 867: 'jukebox',\n",
       " 868: 'across',\n",
       " 869: 'kent',\n",
       " 870: 'seconds',\n",
       " 871: 'dunno',\n",
       " 872: 'loved',\n",
       " 873: 'thirty',\n",
       " 874: 'except',\n",
       " 875: 'set',\n",
       " 876: 'quick',\n",
       " 877: 'gives',\n",
       " 878: 'plastic',\n",
       " 879: 'fall',\n",
       " 880: 'sips',\n",
       " 881: 'early',\n",
       " 882: 'pass',\n",
       " 883: 'romantic',\n",
       " 884: 'hit',\n",
       " 885: 'honey',\n",
       " 886: 'bag',\n",
       " 887: 'heaven',\n",
       " 888: 'rat',\n",
       " 889: 'bet',\n",
       " 890: 'tony',\n",
       " 891: 'charge',\n",
       " 892: '_julius_hibbert:',\n",
       " 893: 'group',\n",
       " 894: 'hair',\n",
       " 895: 'power',\n",
       " 896: 'nigel_bakerbutcher:',\n",
       " 897: 'mister',\n",
       " 898: 'milk',\n",
       " 899: 'plan',\n",
       " 900: 'smells',\n",
       " 901: 'sex',\n",
       " 902: 'star',\n",
       " 903: 'health_inspector:',\n",
       " 904: 'sick',\n",
       " 905: 'company',\n",
       " 906: 'slap',\n",
       " 907: 'sent',\n",
       " 908: 'same',\n",
       " 909: 'health',\n",
       " 910: 'hard',\n",
       " 911: 'gold',\n",
       " 912: 'music',\n",
       " 913: 'walther_hotenhoffer:',\n",
       " 914: 'desperate',\n",
       " 915: 'harv:',\n",
       " 916: 'belch',\n",
       " 917: 'bitter',\n",
       " 918: 'worst',\n",
       " 919: 'along',\n",
       " 920: 'joey',\n",
       " 921: 'holding',\n",
       " 922: \"we'd\",\n",
       " 923: 'water',\n",
       " 924: 'szyslak',\n",
       " 925: 'takes',\n",
       " 926: 'shall',\n",
       " 927: 'treasure',\n",
       " 928: 'laughing',\n",
       " 929: 'lives',\n",
       " 930: 'men',\n",
       " 931: 'class',\n",
       " 932: 'flanders',\n",
       " 933: 'ones',\n",
       " 934: 'denver',\n",
       " 935: 'safe',\n",
       " 936: 'football_announcer:',\n",
       " 937: 'lou:',\n",
       " 938: 'tune',\n",
       " 939: 'ice',\n",
       " 940: 'week',\n",
       " 941: 'slow',\n",
       " 942: 'tinkle',\n",
       " 943: 'won',\n",
       " 944: 'admit',\n",
       " 945: 'maya:',\n",
       " 946: 'truth',\n",
       " 947: \"that'll\",\n",
       " 948: 'numbers',\n",
       " 949: 'birthday',\n",
       " 950: 'became',\n",
       " 951: 'dangerous',\n",
       " 952: 'dumb',\n",
       " 953: 'stool',\n",
       " 954: 'holds',\n",
       " 955: 'gumbel',\n",
       " 956: 'uncle',\n",
       " 957: 'al',\n",
       " 958: 'impressed',\n",
       " 959: 'hank_williams_jr',\n",
       " 960: 'tip',\n",
       " 961: 'evening',\n",
       " 962: 'driving',\n",
       " 963: 'running',\n",
       " 964: 'brilliant',\n",
       " 965: 'jack',\n",
       " 966: 'wall',\n",
       " 967: 'shaking',\n",
       " 968: 'biggest',\n",
       " 969: 'truck',\n",
       " 970: 'lose',\n",
       " 971: 'dank',\n",
       " 972: \"tryin'\",\n",
       " 973: 'nein',\n",
       " 974: 'offended',\n",
       " 975: 'handsome',\n",
       " 976: 'punch',\n",
       " 977: 'counting',\n",
       " 978: 'local',\n",
       " 979: 'broad',\n",
       " 980: 'soul',\n",
       " 981: \"callin'\",\n",
       " 982: 'favor',\n",
       " 983: 'cop',\n",
       " 984: 'beach',\n",
       " 985: 'cleaned',\n",
       " 986: 'testing',\n",
       " 987: 'coaster',\n",
       " 988: 'detective_homer_simpson:',\n",
       " 989: 'surgery',\n",
       " 990: 'against',\n",
       " 991: 'clown',\n",
       " 992: 'dying',\n",
       " 993: 'mine',\n",
       " 994: 'asked',\n",
       " 995: 'billy_the_kid:',\n",
       " 996: 'needs',\n",
       " 997: 'little_man:',\n",
       " 998: 'tv_husband:',\n",
       " 999: 'invited',\n",
       " ...}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标记符号的字符串\n",
    "我们会使用空格当作分隔符，来将剧本分割为词语数组。然而，句号和感叹号等符号使得神经网络难以分辨“再见”和“再见！”之间的区别。\n",
    "\n",
    "实现函数 `token_lookup` 来返回一个字典，这个字典用于将 “!” 等符号标记为 “||Exclamation_Mark||” 形式。为下列符号创建一个字典，其中符号为标志，值为标记。\n",
    "\n",
    "- period ( . )\n",
    "- comma ( , )\n",
    "- quotation mark ( \" )\n",
    "- semicolon ( ; )\n",
    "- exclamation mark ( ! )\n",
    "- question mark ( ? )\n",
    "- left parenthesis ( ( )\n",
    "- right parenthesis ( ) )\n",
    "- dash ( -- )\n",
    "- return ( \\n )\n",
    "\n",
    "这个字典将用于标记符号并在其周围添加分隔符（空格）。这能将符号视作单独词汇分割开来，并使神经网络更轻松地预测下一个词汇。请确保你并没有使用容易与词汇混淆的标记。与其使用 “dash” 这样的标记，试试使用“||dash||”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    token = {'.': '||period||', ',': '||comma||', '\"':'||quotation_mark||', ';':'||semicolon||', \n",
    "             '!': '||exclamation_mark||', '?': '||question_mark||', '(': '||left_parenthesis||', \n",
    "            ')': '||right_parenthesis||', '--': '||dash||', '\\n': '||return||'}\n",
    "    \n",
    "    return token\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理并保存所有数据\n",
    "运行以下代码将预处理所有数据，并将它们保存至文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查点\n",
    "这是你遇到的第一个检点。如果你想要回到这个 notebook，或需要重新打开 notebook，你都可以从这里开始。预处理的数据都已经保存完毕。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建神经网络\n",
    "你将通过实现下面的函数，来创造用于构建 RNN 的必要元素：\n",
    "\n",
    "- get_inputs\n",
    "- get_init\\_cell\n",
    "- get_embed\n",
    "- build_rnn\n",
    "- build_nn\n",
    "- get_batches\n",
    "\n",
    "### 检查 TensorFlow 版本并访问 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.0.1\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入\n",
    "\n",
    "实现函数 `get_inputs()` 来为神经网络创建 TF 占位符。它将创建下列占位符：\n",
    "\n",
    "- 使用 [TF 占位符](https://www.tensorflow.org/api_docs/python/tf/placeholder) `name` 参量输入 \"input\" 文本占位符。\n",
    "- Targets 占位符\n",
    "- Learning Rate 占位符\n",
    "\n",
    "返回下列元组中的占位符 `(Input, Targets, LearningRate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    Input = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    Targets = tf.placeholder(tf.int32, [None, None], name='Targets')\n",
    "    LearningRate = tf.placeholder(tf.float32, name='LearningRate')\n",
    "    output = (Input, Targets, LearningRate)\n",
    "    return output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_inputs(get_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建 RNN Cell 并初始化\n",
    "\n",
    "在 [`MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell) 中堆叠一个或多个 [`BasicLSTMCells`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell)\n",
    "\n",
    "- 使用 `rnn_size` 设定 RNN 大小。\n",
    "- 使用 MultiRNNCell 的 [`zero_state()`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell#zero_state) 函数初始化 Cell 状态\n",
    "- 使用 [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity) 为初始状态应用名称 \"initial_state\"\n",
    " \n",
    "\n",
    "返回 cell 和下列元组中的初始状态 `(Cell, InitialState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    #drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=LearningRate)\n",
    "    Cell = tf.contrib.rnn.MultiRNNCell([lstm] * 1)\n",
    "    InitialState = Cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    InitialState = tf.identity(InitialState,name='initial_state')\n",
    "    return Cell, InitialState\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_init_cell(get_init_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词嵌入\n",
    "使用 TensorFlow 将嵌入运用到 `input_data` 中。\n",
    "返回嵌入序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))  # create embedding weight matrix here\n",
    "    # vocab_size是词数 ，embed_dim是每个embed的size\n",
    "    embed = tf.nn.embedding_lookup(embedding, input_data) # use tf.nn.embedding_lookup to get the hidden layer output\n",
    "    \n",
    "    return embed\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_embed(get_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建 RNN\n",
    "你已经在 `get_init_cell()` 函数中创建了 RNN Cell。是时候使用这个 Cell 来创建 RNN了。\n",
    "\n",
    "- 使用 [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn) 创建 RNN\n",
    "- 使用 [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity) 将名称 \"final_state\" 应用到最终状态中\n",
    "\n",
    "\n",
    "返回下列元组中的输出和最终状态`(Outputs, FinalState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    Outputs, Final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    Final_state = tf.identity(Final_state,name='final_state')\n",
    "    return Outputs, Final_state\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_rnn(build_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建神经网络\n",
    "应用你在上面实现的函数，来：\n",
    "\n",
    "- 使用你的 `get_embed(input_data, vocab_size, embed_dim)` 函数将嵌入应用到 `input_data` 中\n",
    "- 使用 `cell` 和你的 `build_rnn(cell, inputs)` 函数来创建 RNN\n",
    "- 应用一个完全联通线性激活和 `vocab_size` 的分层作为输出数量。\n",
    "\n",
    "返回下列元组中的 logit 和最终状态 `Logits, FinalState`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    embed = get_embed(input_data, vocab_size, embed_dim)\n",
    "    #Cell, InitialState  =  get_init_cell(batch_size, rnn_size)\n",
    "    outputs, FinalState = build_rnn(cell, embed)\n",
    "    Logits = tf.contrib.layers.fully_connected(inputs=outputs, num_outputs=vocab_size, activation_fn=None)\n",
    "    return Logits, FinalState\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_nn(build_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批次\n",
    "\n",
    "实现 `get_batches` 来使用 `int_text` 创建输入与目标批次。这些批次应为 Numpy 数组，并具有形状 `(number of batches, 2, batch size, sequence length)`。每个批次包含两个元素：\n",
    "\n",
    "- 第一个元素为**输入**的单独批次，并具有形状 `[batch size, sequence length]`\n",
    "- 第二个元素为**目标**的单独批次，并具有形状 `[batch size, sequence length]`\n",
    "\n",
    "如果你无法在最后一个批次中填入足够数据，请放弃这个批次。\n",
    "\n",
    "例如 `get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], 2, 3)` 将返回下面这个 Numpy 数组："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-39-585e60cf595e>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-39-585e60cf595e>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    [[ 1  2  3], [ 7  8  9]],\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[\n",
    "  # First Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 1  2  3], [ 7  8  9]],\n",
    "    # Batch of targets\n",
    "    [[ 2  3  4], [ 8  9 10]]\n",
    "  ],\n",
    " \n",
    "  # Second Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 4  5  6], [10 11 12]],\n",
    "    # Batch of targets\n",
    "    [[ 5  6  7], [11 12 13]]\n",
    "  ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69100"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    n_batches = len(int_text)//(batch_size * seq_length)\n",
    "    x_input = np.array(int_text[:n_batches * batch_size * seq_length ])\n",
    "    y_targets = np.array(int_text[1:n_batches * batch_size * seq_length + 1])\n",
    "    \n",
    "    x_batches = np.split(x_input.reshape(batch_size, -1), n_batches, 1)\n",
    "    y_batches = np.split(y_targets.reshape(batch_size, -1), n_batches, 1)\n",
    "    \n",
    "    x_batches_shape = np.array(x_batches).reshape(n_batches,1,batch_size, seq_length)\n",
    "    y_batches_shape = np.array(y_batches).reshape(n_batches,1,batch_size, seq_length)\n",
    "    \n",
    "    final_batch = np.concatenate([x_batches_shape,y_batches_shape], axis = 1)\n",
    "    return final_batch\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_batches(get_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络训练\n",
    "### 超参数\n",
    "调整下列参数:\n",
    "\n",
    "- 将 `num_epochs` 设置为训练次数。\n",
    "- 将 `batch_size` 设置为程序组大小。\n",
    "- 将 `rnn_size` 设置为 RNN 大小。\n",
    "- 将 `embed_dim` 设置为嵌入大小。\n",
    "- 将 `seq_length` 设置为序列长度。\n",
    "- 将 `learning_rate` 设置为学习率。\n",
    "- 将 `show_every_n_batches` 设置为神经网络应输出的程序组数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 200\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# RNN Size\n",
    "rnn_size = 600\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 300\n",
    "# Sequence Length\n",
    "seq_length = 11\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 10\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建图表\n",
    "使用你实现的神经网络创建图表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "在预处理数据中训练神经网络。如果你遇到困难，请查看这个[表格](https://discussions.udacity.com/)，看看是否有人遇到了和你一样的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/49   train_loss = 8.820\n",
      "Epoch   0 Batch   10/49   train_loss = 6.918\n",
      "Epoch   0 Batch   20/49   train_loss = 6.628\n",
      "Epoch   0 Batch   30/49   train_loss = 6.207\n",
      "Epoch   0 Batch   40/49   train_loss = 5.929\n",
      "Epoch   1 Batch    1/49   train_loss = 5.874\n",
      "Epoch   1 Batch   11/49   train_loss = 5.644\n",
      "Epoch   1 Batch   21/49   train_loss = 5.642\n",
      "Epoch   1 Batch   31/49   train_loss = 5.434\n",
      "Epoch   1 Batch   41/49   train_loss = 5.251\n",
      "Epoch   2 Batch    2/49   train_loss = 5.315\n",
      "Epoch   2 Batch   12/49   train_loss = 5.209\n",
      "Epoch   2 Batch   22/49   train_loss = 5.200\n",
      "Epoch   2 Batch   32/49   train_loss = 5.116\n",
      "Epoch   2 Batch   42/49   train_loss = 4.992\n",
      "Epoch   3 Batch    3/49   train_loss = 4.988\n",
      "Epoch   3 Batch   13/49   train_loss = 4.908\n",
      "Epoch   3 Batch   23/49   train_loss = 4.782\n",
      "Epoch   3 Batch   33/49   train_loss = 4.840\n",
      "Epoch   3 Batch   43/49   train_loss = 5.060\n",
      "Epoch   4 Batch    4/49   train_loss = 4.797\n",
      "Epoch   4 Batch   14/49   train_loss = 4.832\n",
      "Epoch   4 Batch   24/49   train_loss = 4.676\n",
      "Epoch   4 Batch   34/49   train_loss = 4.666\n",
      "Epoch   4 Batch   44/49   train_loss = 4.645\n",
      "Epoch   5 Batch    5/49   train_loss = 4.609\n",
      "Epoch   5 Batch   15/49   train_loss = 4.738\n",
      "Epoch   5 Batch   25/49   train_loss = 4.702\n",
      "Epoch   5 Batch   35/49   train_loss = 4.563\n",
      "Epoch   5 Batch   45/49   train_loss = 4.575\n",
      "Epoch   6 Batch    6/49   train_loss = 4.458\n",
      "Epoch   6 Batch   16/49   train_loss = 4.418\n",
      "Epoch   6 Batch   26/49   train_loss = 4.338\n",
      "Epoch   6 Batch   36/49   train_loss = 4.467\n",
      "Epoch   6 Batch   46/49   train_loss = 4.290\n",
      "Epoch   7 Batch    7/49   train_loss = 4.273\n",
      "Epoch   7 Batch   17/49   train_loss = 4.215\n",
      "Epoch   7 Batch   27/49   train_loss = 4.253\n",
      "Epoch   7 Batch   37/49   train_loss = 4.057\n",
      "Epoch   7 Batch   47/49   train_loss = 4.011\n",
      "Epoch   8 Batch    8/49   train_loss = 4.175\n",
      "Epoch   8 Batch   18/49   train_loss = 4.057\n",
      "Epoch   8 Batch   28/49   train_loss = 4.045\n",
      "Epoch   8 Batch   38/49   train_loss = 4.087\n",
      "Epoch   8 Batch   48/49   train_loss = 3.991\n",
      "Epoch   9 Batch    9/49   train_loss = 4.055\n",
      "Epoch   9 Batch   19/49   train_loss = 3.953\n",
      "Epoch   9 Batch   29/49   train_loss = 3.802\n",
      "Epoch   9 Batch   39/49   train_loss = 3.900\n",
      "Epoch  10 Batch    0/49   train_loss = 3.760\n",
      "Epoch  10 Batch   10/49   train_loss = 3.955\n",
      "Epoch  10 Batch   20/49   train_loss = 3.862\n",
      "Epoch  10 Batch   30/49   train_loss = 3.736\n",
      "Epoch  10 Batch   40/49   train_loss = 3.603\n",
      "Epoch  11 Batch    1/49   train_loss = 3.739\n",
      "Epoch  11 Batch   11/49   train_loss = 3.775\n",
      "Epoch  11 Batch   21/49   train_loss = 3.771\n",
      "Epoch  11 Batch   31/49   train_loss = 3.559\n",
      "Epoch  11 Batch   41/49   train_loss = 3.530\n",
      "Epoch  12 Batch    2/49   train_loss = 3.541\n",
      "Epoch  12 Batch   12/49   train_loss = 3.561\n",
      "Epoch  12 Batch   22/49   train_loss = 3.566\n",
      "Epoch  12 Batch   32/49   train_loss = 3.466\n",
      "Epoch  12 Batch   42/49   train_loss = 3.421\n",
      "Epoch  13 Batch    3/49   train_loss = 3.426\n",
      "Epoch  13 Batch   13/49   train_loss = 3.450\n",
      "Epoch  13 Batch   23/49   train_loss = 3.241\n",
      "Epoch  13 Batch   33/49   train_loss = 3.343\n",
      "Epoch  13 Batch   43/49   train_loss = 3.464\n",
      "Epoch  14 Batch    4/49   train_loss = 3.327\n",
      "Epoch  14 Batch   14/49   train_loss = 3.297\n",
      "Epoch  14 Batch   24/49   train_loss = 3.239\n",
      "Epoch  14 Batch   34/49   train_loss = 3.264\n",
      "Epoch  14 Batch   44/49   train_loss = 3.184\n",
      "Epoch  15 Batch    5/49   train_loss = 3.136\n",
      "Epoch  15 Batch   15/49   train_loss = 3.240\n",
      "Epoch  15 Batch   25/49   train_loss = 3.202\n",
      "Epoch  15 Batch   35/49   train_loss = 3.115\n",
      "Epoch  15 Batch   45/49   train_loss = 3.162\n",
      "Epoch  16 Batch    6/49   train_loss = 3.062\n",
      "Epoch  16 Batch   16/49   train_loss = 3.017\n",
      "Epoch  16 Batch   26/49   train_loss = 2.981\n",
      "Epoch  16 Batch   36/49   train_loss = 2.991\n",
      "Epoch  16 Batch   46/49   train_loss = 2.896\n",
      "Epoch  17 Batch    7/49   train_loss = 3.021\n",
      "Epoch  17 Batch   17/49   train_loss = 2.922\n",
      "Epoch  17 Batch   27/49   train_loss = 2.876\n",
      "Epoch  17 Batch   37/49   train_loss = 2.804\n",
      "Epoch  17 Batch   47/49   train_loss = 2.793\n",
      "Epoch  18 Batch    8/49   train_loss = 2.851\n",
      "Epoch  18 Batch   18/49   train_loss = 2.741\n",
      "Epoch  18 Batch   28/49   train_loss = 2.817\n",
      "Epoch  18 Batch   38/49   train_loss = 2.744\n",
      "Epoch  18 Batch   48/49   train_loss = 2.737\n",
      "Epoch  19 Batch    9/49   train_loss = 2.719\n",
      "Epoch  19 Batch   19/49   train_loss = 2.733\n",
      "Epoch  19 Batch   29/49   train_loss = 2.628\n",
      "Epoch  19 Batch   39/49   train_loss = 2.555\n",
      "Epoch  20 Batch    0/49   train_loss = 2.586\n",
      "Epoch  20 Batch   10/49   train_loss = 2.601\n",
      "Epoch  20 Batch   20/49   train_loss = 2.611\n",
      "Epoch  20 Batch   30/49   train_loss = 2.583\n",
      "Epoch  20 Batch   40/49   train_loss = 2.451\n",
      "Epoch  21 Batch    1/49   train_loss = 2.496\n",
      "Epoch  21 Batch   11/49   train_loss = 2.602\n",
      "Epoch  21 Batch   21/49   train_loss = 2.530\n",
      "Epoch  21 Batch   31/49   train_loss = 2.376\n",
      "Epoch  21 Batch   41/49   train_loss = 2.420\n",
      "Epoch  22 Batch    2/49   train_loss = 2.392\n",
      "Epoch  22 Batch   12/49   train_loss = 2.372\n",
      "Epoch  22 Batch   22/49   train_loss = 2.351\n",
      "Epoch  22 Batch   32/49   train_loss = 2.369\n",
      "Epoch  22 Batch   42/49   train_loss = 2.347\n",
      "Epoch  23 Batch    3/49   train_loss = 2.349\n",
      "Epoch  23 Batch   13/49   train_loss = 2.318\n",
      "Epoch  23 Batch   23/49   train_loss = 2.142\n",
      "Epoch  23 Batch   33/49   train_loss = 2.274\n",
      "Epoch  23 Batch   43/49   train_loss = 2.285\n",
      "Epoch  24 Batch    4/49   train_loss = 2.240\n",
      "Epoch  24 Batch   14/49   train_loss = 2.137\n",
      "Epoch  24 Batch   24/49   train_loss = 2.205\n",
      "Epoch  24 Batch   34/49   train_loss = 2.189\n",
      "Epoch  24 Batch   44/49   train_loss = 2.126\n",
      "Epoch  25 Batch    5/49   train_loss = 2.117\n",
      "Epoch  25 Batch   15/49   train_loss = 2.166\n",
      "Epoch  25 Batch   25/49   train_loss = 2.101\n",
      "Epoch  25 Batch   35/49   train_loss = 2.090\n",
      "Epoch  25 Batch   45/49   train_loss = 2.074\n",
      "Epoch  26 Batch    6/49   train_loss = 2.042\n",
      "Epoch  26 Batch   16/49   train_loss = 1.967\n",
      "Epoch  26 Batch   26/49   train_loss = 1.987\n",
      "Epoch  26 Batch   36/49   train_loss = 1.928\n",
      "Epoch  26 Batch   46/49   train_loss = 1.876\n",
      "Epoch  27 Batch    7/49   train_loss = 1.999\n",
      "Epoch  27 Batch   17/49   train_loss = 1.932\n",
      "Epoch  27 Batch   27/49   train_loss = 1.836\n",
      "Epoch  27 Batch   37/49   train_loss = 1.824\n",
      "Epoch  27 Batch   47/49   train_loss = 1.809\n",
      "Epoch  28 Batch    8/49   train_loss = 1.815\n",
      "Epoch  28 Batch   18/49   train_loss = 1.768\n",
      "Epoch  28 Batch   28/49   train_loss = 1.809\n",
      "Epoch  28 Batch   38/49   train_loss = 1.672\n",
      "Epoch  28 Batch   48/49   train_loss = 1.692\n",
      "Epoch  29 Batch    9/49   train_loss = 1.665\n",
      "Epoch  29 Batch   19/49   train_loss = 1.788\n",
      "Epoch  29 Batch   29/49   train_loss = 1.743\n",
      "Epoch  29 Batch   39/49   train_loss = 1.579\n",
      "Epoch  30 Batch    0/49   train_loss = 1.661\n",
      "Epoch  30 Batch   10/49   train_loss = 1.617\n",
      "Epoch  30 Batch   20/49   train_loss = 1.677\n",
      "Epoch  30 Batch   30/49   train_loss = 1.691\n",
      "Epoch  30 Batch   40/49   train_loss = 1.565\n",
      "Epoch  31 Batch    1/49   train_loss = 1.543\n",
      "Epoch  31 Batch   11/49   train_loss = 1.647\n",
      "Epoch  31 Batch   21/49   train_loss = 1.596\n",
      "Epoch  31 Batch   31/49   train_loss = 1.542\n",
      "Epoch  31 Batch   41/49   train_loss = 1.542\n",
      "Epoch  32 Batch    2/49   train_loss = 1.484\n",
      "Epoch  32 Batch   12/49   train_loss = 1.515\n",
      "Epoch  32 Batch   22/49   train_loss = 1.441\n",
      "Epoch  32 Batch   32/49   train_loss = 1.485\n",
      "Epoch  32 Batch   42/49   train_loss = 1.456\n",
      "Epoch  33 Batch    3/49   train_loss = 1.449\n",
      "Epoch  33 Batch   13/49   train_loss = 1.471\n",
      "Epoch  33 Batch   23/49   train_loss = 1.309\n",
      "Epoch  33 Batch   33/49   train_loss = 1.427\n",
      "Epoch  33 Batch   43/49   train_loss = 1.335\n",
      "Epoch  34 Batch    4/49   train_loss = 1.288\n",
      "Epoch  34 Batch   14/49   train_loss = 1.322\n",
      "Epoch  34 Batch   24/49   train_loss = 1.370\n",
      "Epoch  34 Batch   34/49   train_loss = 1.345\n",
      "Epoch  34 Batch   44/49   train_loss = 1.243\n",
      "Epoch  35 Batch    5/49   train_loss = 1.287\n",
      "Epoch  35 Batch   15/49   train_loss = 1.304\n",
      "Epoch  35 Batch   25/49   train_loss = 1.237\n",
      "Epoch  35 Batch   35/49   train_loss = 1.284\n",
      "Epoch  35 Batch   45/49   train_loss = 1.238\n",
      "Epoch  36 Batch    6/49   train_loss = 1.191\n",
      "Epoch  36 Batch   16/49   train_loss = 1.193\n",
      "Epoch  36 Batch   26/49   train_loss = 1.232\n",
      "Epoch  36 Batch   36/49   train_loss = 1.157\n",
      "Epoch  36 Batch   46/49   train_loss = 1.124\n",
      "Epoch  37 Batch    7/49   train_loss = 1.182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  37 Batch   17/49   train_loss = 1.195\n",
      "Epoch  37 Batch   27/49   train_loss = 1.131\n",
      "Epoch  37 Batch   37/49   train_loss = 1.141\n",
      "Epoch  37 Batch   47/49   train_loss = 1.111\n",
      "Epoch  38 Batch    8/49   train_loss = 1.039\n",
      "Epoch  38 Batch   18/49   train_loss = 1.100\n",
      "Epoch  38 Batch   28/49   train_loss = 1.098\n",
      "Epoch  38 Batch   38/49   train_loss = 1.035\n",
      "Epoch  38 Batch   48/49   train_loss = 1.035\n",
      "Epoch  39 Batch    9/49   train_loss = 0.942\n",
      "Epoch  39 Batch   19/49   train_loss = 1.040\n",
      "Epoch  39 Batch   29/49   train_loss = 1.079\n",
      "Epoch  39 Batch   39/49   train_loss = 0.942\n",
      "Epoch  40 Batch    0/49   train_loss = 1.038\n",
      "Epoch  40 Batch   10/49   train_loss = 0.951\n",
      "Epoch  40 Batch   20/49   train_loss = 0.995\n",
      "Epoch  40 Batch   30/49   train_loss = 1.030\n",
      "Epoch  40 Batch   40/49   train_loss = 1.010\n",
      "Epoch  41 Batch    1/49   train_loss = 0.951\n",
      "Epoch  41 Batch   11/49   train_loss = 0.994\n",
      "Epoch  41 Batch   21/49   train_loss = 0.991\n",
      "Epoch  41 Batch   31/49   train_loss = 0.982\n",
      "Epoch  41 Batch   41/49   train_loss = 0.963\n",
      "Epoch  42 Batch    2/49   train_loss = 0.956\n",
      "Epoch  42 Batch   12/49   train_loss = 0.928\n",
      "Epoch  42 Batch   22/49   train_loss = 0.887\n",
      "Epoch  42 Batch   32/49   train_loss = 0.927\n",
      "Epoch  42 Batch   42/49   train_loss = 0.895\n",
      "Epoch  43 Batch    3/49   train_loss = 0.921\n",
      "Epoch  43 Batch   13/49   train_loss = 0.914\n",
      "Epoch  43 Batch   23/49   train_loss = 0.803\n",
      "Epoch  43 Batch   33/49   train_loss = 0.881\n",
      "Epoch  43 Batch   43/49   train_loss = 0.787\n",
      "Epoch  44 Batch    4/49   train_loss = 0.808\n",
      "Epoch  44 Batch   14/49   train_loss = 0.808\n",
      "Epoch  44 Batch   24/49   train_loss = 0.876\n",
      "Epoch  44 Batch   34/49   train_loss = 0.837\n",
      "Epoch  44 Batch   44/49   train_loss = 0.761\n",
      "Epoch  45 Batch    5/49   train_loss = 0.817\n",
      "Epoch  45 Batch   15/49   train_loss = 0.806\n",
      "Epoch  45 Batch   25/49   train_loss = 0.746\n",
      "Epoch  45 Batch   35/49   train_loss = 0.783\n",
      "Epoch  45 Batch   45/49   train_loss = 0.745\n",
      "Epoch  46 Batch    6/49   train_loss = 0.707\n",
      "Epoch  46 Batch   16/49   train_loss = 0.735\n",
      "Epoch  46 Batch   26/49   train_loss = 0.750\n",
      "Epoch  46 Batch   36/49   train_loss = 0.710\n",
      "Epoch  46 Batch   46/49   train_loss = 0.697\n",
      "Epoch  47 Batch    7/49   train_loss = 0.722\n",
      "Epoch  47 Batch   17/49   train_loss = 0.730\n",
      "Epoch  47 Batch   27/49   train_loss = 0.690\n",
      "Epoch  47 Batch   37/49   train_loss = 0.713\n",
      "Epoch  47 Batch   47/49   train_loss = 0.686\n",
      "Epoch  48 Batch    8/49   train_loss = 0.619\n",
      "Epoch  48 Batch   18/49   train_loss = 0.679\n",
      "Epoch  48 Batch   28/49   train_loss = 0.659\n",
      "Epoch  48 Batch   38/49   train_loss = 0.613\n",
      "Epoch  48 Batch   48/49   train_loss = 0.661\n",
      "Epoch  49 Batch    9/49   train_loss = 0.586\n",
      "Epoch  49 Batch   19/49   train_loss = 0.606\n",
      "Epoch  49 Batch   29/49   train_loss = 0.680\n",
      "Epoch  49 Batch   39/49   train_loss = 0.561\n",
      "Epoch  50 Batch    0/49   train_loss = 0.648\n",
      "Epoch  50 Batch   10/49   train_loss = 0.585\n",
      "Epoch  50 Batch   20/49   train_loss = 0.626\n",
      "Epoch  50 Batch   30/49   train_loss = 0.642\n",
      "Epoch  50 Batch   40/49   train_loss = 0.637\n",
      "Epoch  51 Batch    1/49   train_loss = 0.586\n",
      "Epoch  51 Batch   11/49   train_loss = 0.626\n",
      "Epoch  51 Batch   21/49   train_loss = 0.617\n",
      "Epoch  51 Batch   31/49   train_loss = 0.628\n",
      "Epoch  51 Batch   41/49   train_loss = 0.588\n",
      "Epoch  52 Batch    2/49   train_loss = 0.593\n",
      "Epoch  52 Batch   12/49   train_loss = 0.554\n",
      "Epoch  52 Batch   22/49   train_loss = 0.545\n",
      "Epoch  52 Batch   32/49   train_loss = 0.594\n",
      "Epoch  52 Batch   42/49   train_loss = 0.546\n",
      "Epoch  53 Batch    3/49   train_loss = 0.563\n",
      "Epoch  53 Batch   13/49   train_loss = 0.552\n",
      "Epoch  53 Batch   23/49   train_loss = 0.487\n",
      "Epoch  53 Batch   33/49   train_loss = 0.563\n",
      "Epoch  53 Batch   43/49   train_loss = 0.513\n",
      "Epoch  54 Batch    4/49   train_loss = 0.510\n",
      "Epoch  54 Batch   14/49   train_loss = 0.512\n",
      "Epoch  54 Batch   24/49   train_loss = 0.563\n",
      "Epoch  54 Batch   34/49   train_loss = 0.516\n",
      "Epoch  54 Batch   44/49   train_loss = 0.483\n",
      "Epoch  55 Batch    5/49   train_loss = 0.557\n",
      "Epoch  55 Batch   15/49   train_loss = 0.529\n",
      "Epoch  55 Batch   25/49   train_loss = 0.506\n",
      "Epoch  55 Batch   35/49   train_loss = 0.524\n",
      "Epoch  55 Batch   45/49   train_loss = 0.497\n",
      "Epoch  56 Batch    6/49   train_loss = 0.488\n",
      "Epoch  56 Batch   16/49   train_loss = 0.475\n",
      "Epoch  56 Batch   26/49   train_loss = 0.507\n",
      "Epoch  56 Batch   36/49   train_loss = 0.489\n",
      "Epoch  56 Batch   46/49   train_loss = 0.460\n",
      "Epoch  57 Batch    7/49   train_loss = 0.496\n",
      "Epoch  57 Batch   17/49   train_loss = 0.469\n",
      "Epoch  57 Batch   27/49   train_loss = 0.477\n",
      "Epoch  57 Batch   37/49   train_loss = 0.503\n",
      "Epoch  57 Batch   47/49   train_loss = 0.478\n",
      "Epoch  58 Batch    8/49   train_loss = 0.437\n",
      "Epoch  58 Batch   18/49   train_loss = 0.480\n",
      "Epoch  58 Batch   28/49   train_loss = 0.445\n",
      "Epoch  58 Batch   38/49   train_loss = 0.442\n",
      "Epoch  58 Batch   48/49   train_loss = 0.470\n",
      "Epoch  59 Batch    9/49   train_loss = 0.427\n",
      "Epoch  59 Batch   19/49   train_loss = 0.415\n",
      "Epoch  59 Batch   29/49   train_loss = 0.477\n",
      "Epoch  59 Batch   39/49   train_loss = 0.412\n",
      "Epoch  60 Batch    0/49   train_loss = 0.469\n",
      "Epoch  60 Batch   10/49   train_loss = 0.432\n",
      "Epoch  60 Batch   20/49   train_loss = 0.444\n",
      "Epoch  60 Batch   30/49   train_loss = 0.463\n",
      "Epoch  60 Batch   40/49   train_loss = 0.487\n",
      "Epoch  61 Batch    1/49   train_loss = 0.439\n",
      "Epoch  61 Batch   11/49   train_loss = 0.473\n",
      "Epoch  61 Batch   21/49   train_loss = 0.440\n",
      "Epoch  61 Batch   31/49   train_loss = 0.466\n",
      "Epoch  61 Batch   41/49   train_loss = 0.438\n",
      "Epoch  62 Batch    2/49   train_loss = 0.444\n",
      "Epoch  62 Batch   12/49   train_loss = 0.411\n",
      "Epoch  62 Batch   22/49   train_loss = 0.407\n",
      "Epoch  62 Batch   32/49   train_loss = 0.444\n",
      "Epoch  62 Batch   42/49   train_loss = 0.409\n",
      "Epoch  63 Batch    3/49   train_loss = 0.405\n",
      "Epoch  63 Batch   13/49   train_loss = 0.420\n",
      "Epoch  63 Batch   23/49   train_loss = 0.361\n",
      "Epoch  63 Batch   33/49   train_loss = 0.417\n",
      "Epoch  63 Batch   43/49   train_loss = 0.393\n",
      "Epoch  64 Batch    4/49   train_loss = 0.393\n",
      "Epoch  64 Batch   14/49   train_loss = 0.392\n",
      "Epoch  64 Batch   24/49   train_loss = 0.434\n",
      "Epoch  64 Batch   34/49   train_loss = 0.387\n",
      "Epoch  64 Batch   44/49   train_loss = 0.367\n",
      "Epoch  65 Batch    5/49   train_loss = 0.417\n",
      "Epoch  65 Batch   15/49   train_loss = 0.421\n",
      "Epoch  65 Batch   25/49   train_loss = 0.397\n",
      "Epoch  65 Batch   35/49   train_loss = 0.414\n",
      "Epoch  65 Batch   45/49   train_loss = 0.384\n",
      "Epoch  66 Batch    6/49   train_loss = 0.383\n",
      "Epoch  66 Batch   16/49   train_loss = 0.377\n",
      "Epoch  66 Batch   26/49   train_loss = 0.406\n",
      "Epoch  66 Batch   36/49   train_loss = 0.384\n",
      "Epoch  66 Batch   46/49   train_loss = 0.355\n",
      "Epoch  67 Batch    7/49   train_loss = 0.388\n",
      "Epoch  67 Batch   17/49   train_loss = 0.382\n",
      "Epoch  67 Batch   27/49   train_loss = 0.384\n",
      "Epoch  67 Batch   37/49   train_loss = 0.391\n",
      "Epoch  67 Batch   47/49   train_loss = 0.368\n",
      "Epoch  68 Batch    8/49   train_loss = 0.349\n",
      "Epoch  68 Batch   18/49   train_loss = 0.399\n",
      "Epoch  68 Batch   28/49   train_loss = 0.354\n",
      "Epoch  68 Batch   38/49   train_loss = 0.362\n",
      "Epoch  68 Batch   48/49   train_loss = 0.375\n",
      "Epoch  69 Batch    9/49   train_loss = 0.339\n",
      "Epoch  69 Batch   19/49   train_loss = 0.344\n",
      "Epoch  69 Batch   29/49   train_loss = 0.396\n",
      "Epoch  69 Batch   39/49   train_loss = 0.341\n",
      "Epoch  70 Batch    0/49   train_loss = 0.399\n",
      "Epoch  70 Batch   10/49   train_loss = 0.369\n",
      "Epoch  70 Batch   20/49   train_loss = 0.377\n",
      "Epoch  70 Batch   30/49   train_loss = 0.386\n",
      "Epoch  70 Batch   40/49   train_loss = 0.382\n",
      "Epoch  71 Batch    1/49   train_loss = 0.379\n",
      "Epoch  71 Batch   11/49   train_loss = 0.404\n",
      "Epoch  71 Batch   21/49   train_loss = 0.362\n",
      "Epoch  71 Batch   31/49   train_loss = 0.402\n",
      "Epoch  71 Batch   41/49   train_loss = 0.355\n",
      "Epoch  72 Batch    2/49   train_loss = 0.390\n",
      "Epoch  72 Batch   12/49   train_loss = 0.353\n",
      "Epoch  72 Batch   22/49   train_loss = 0.350\n",
      "Epoch  72 Batch   32/49   train_loss = 0.383\n",
      "Epoch  72 Batch   42/49   train_loss = 0.338\n",
      "Epoch  73 Batch    3/49   train_loss = 0.351\n",
      "Epoch  73 Batch   13/49   train_loss = 0.360\n",
      "Epoch  73 Batch   23/49   train_loss = 0.313\n",
      "Epoch  73 Batch   33/49   train_loss = 0.369\n",
      "Epoch  73 Batch   43/49   train_loss = 0.345\n",
      "Epoch  74 Batch    4/49   train_loss = 0.344\n",
      "Epoch  74 Batch   14/49   train_loss = 0.337\n",
      "Epoch  74 Batch   24/49   train_loss = 0.389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  74 Batch   34/49   train_loss = 0.343\n",
      "Epoch  74 Batch   44/49   train_loss = 0.319\n",
      "Epoch  75 Batch    5/49   train_loss = 0.360\n",
      "Epoch  75 Batch   15/49   train_loss = 0.374\n",
      "Epoch  75 Batch   25/49   train_loss = 0.359\n",
      "Epoch  75 Batch   35/49   train_loss = 0.374\n",
      "Epoch  75 Batch   45/49   train_loss = 0.344\n",
      "Epoch  76 Batch    6/49   train_loss = 0.347\n",
      "Epoch  76 Batch   16/49   train_loss = 0.330\n",
      "Epoch  76 Batch   26/49   train_loss = 0.368\n",
      "Epoch  76 Batch   36/49   train_loss = 0.352\n",
      "Epoch  76 Batch   46/49   train_loss = 0.319\n",
      "Epoch  77 Batch    7/49   train_loss = 0.350\n",
      "Epoch  77 Batch   17/49   train_loss = 0.347\n",
      "Epoch  77 Batch   27/49   train_loss = 0.348\n",
      "Epoch  77 Batch   37/49   train_loss = 0.363\n",
      "Epoch  77 Batch   47/49   train_loss = 0.335\n",
      "Epoch  78 Batch    8/49   train_loss = 0.322\n",
      "Epoch  78 Batch   18/49   train_loss = 0.369\n",
      "Epoch  78 Batch   28/49   train_loss = 0.320\n",
      "Epoch  78 Batch   38/49   train_loss = 0.338\n",
      "Epoch  78 Batch   48/49   train_loss = 0.344\n",
      "Epoch  79 Batch    9/49   train_loss = 0.309\n",
      "Epoch  79 Batch   19/49   train_loss = 0.319\n",
      "Epoch  79 Batch   29/49   train_loss = 0.364\n",
      "Epoch  79 Batch   39/49   train_loss = 0.322\n",
      "Epoch  80 Batch    0/49   train_loss = 0.366\n",
      "Epoch  80 Batch   10/49   train_loss = 0.342\n",
      "Epoch  80 Batch   20/49   train_loss = 0.349\n",
      "Epoch  80 Batch   30/49   train_loss = 0.362\n",
      "Epoch  80 Batch   40/49   train_loss = 0.353\n",
      "Epoch  81 Batch    1/49   train_loss = 0.353\n",
      "Epoch  81 Batch   11/49   train_loss = 0.375\n",
      "Epoch  81 Batch   21/49   train_loss = 0.341\n",
      "Epoch  81 Batch   31/49   train_loss = 0.376\n",
      "Epoch  81 Batch   41/49   train_loss = 0.335\n",
      "Epoch  82 Batch    2/49   train_loss = 0.366\n",
      "Epoch  82 Batch   12/49   train_loss = 0.329\n",
      "Epoch  82 Batch   22/49   train_loss = 0.333\n",
      "Epoch  82 Batch   32/49   train_loss = 0.365\n",
      "Epoch  82 Batch   42/49   train_loss = 0.318\n",
      "Epoch  83 Batch    3/49   train_loss = 0.332\n",
      "Epoch  83 Batch   13/49   train_loss = 0.338\n",
      "Epoch  83 Batch   23/49   train_loss = 0.296\n",
      "Epoch  83 Batch   33/49   train_loss = 0.350\n",
      "Epoch  83 Batch   43/49   train_loss = 0.332\n",
      "Epoch  84 Batch    4/49   train_loss = 0.330\n",
      "Epoch  84 Batch   14/49   train_loss = 0.323\n",
      "Epoch  84 Batch   24/49   train_loss = 0.372\n",
      "Epoch  84 Batch   34/49   train_loss = 0.328\n",
      "Epoch  84 Batch   44/49   train_loss = 0.308\n",
      "Epoch  85 Batch    5/49   train_loss = 0.348\n",
      "Epoch  85 Batch   15/49   train_loss = 0.358\n",
      "Epoch  85 Batch   25/49   train_loss = 0.343\n",
      "Epoch  85 Batch   35/49   train_loss = 0.361\n",
      "Epoch  85 Batch   45/49   train_loss = 0.333\n",
      "Epoch  86 Batch    6/49   train_loss = 0.338\n",
      "Epoch  86 Batch   16/49   train_loss = 0.319\n",
      "Epoch  86 Batch   26/49   train_loss = 0.354\n",
      "Epoch  86 Batch   36/49   train_loss = 0.341\n",
      "Epoch  86 Batch   46/49   train_loss = 0.305\n",
      "Epoch  87 Batch    7/49   train_loss = 0.340\n",
      "Epoch  87 Batch   17/49   train_loss = 0.335\n",
      "Epoch  87 Batch   27/49   train_loss = 0.336\n",
      "Epoch  87 Batch   37/49   train_loss = 0.355\n",
      "Epoch  87 Batch   47/49   train_loss = 0.324\n",
      "Epoch  88 Batch    8/49   train_loss = 0.313\n",
      "Epoch  88 Batch   18/49   train_loss = 0.354\n",
      "Epoch  88 Batch   28/49   train_loss = 0.309\n",
      "Epoch  88 Batch   38/49   train_loss = 0.331\n",
      "Epoch  88 Batch   48/49   train_loss = 0.337\n",
      "Epoch  89 Batch    9/49   train_loss = 0.299\n",
      "Epoch  89 Batch   19/49   train_loss = 0.307\n",
      "Epoch  89 Batch   29/49   train_loss = 0.354\n",
      "Epoch  89 Batch   39/49   train_loss = 0.314\n",
      "Epoch  90 Batch    0/49   train_loss = 0.356\n",
      "Epoch  90 Batch   10/49   train_loss = 0.333\n",
      "Epoch  90 Batch   20/49   train_loss = 0.339\n",
      "Epoch  90 Batch   30/49   train_loss = 0.353\n",
      "Epoch  90 Batch   40/49   train_loss = 0.349\n",
      "Epoch  91 Batch    1/49   train_loss = 0.343\n",
      "Epoch  91 Batch   11/49   train_loss = 0.370\n",
      "Epoch  91 Batch   21/49   train_loss = 0.332\n",
      "Epoch  91 Batch   31/49   train_loss = 0.366\n",
      "Epoch  91 Batch   41/49   train_loss = 0.329\n",
      "Epoch  92 Batch    2/49   train_loss = 0.355\n",
      "Epoch  92 Batch   12/49   train_loss = 0.322\n",
      "Epoch  92 Batch   22/49   train_loss = 0.327\n",
      "Epoch  92 Batch   32/49   train_loss = 0.358\n",
      "Epoch  92 Batch   42/49   train_loss = 0.313\n",
      "Epoch  93 Batch    3/49   train_loss = 0.324\n",
      "Epoch  93 Batch   13/49   train_loss = 0.327\n",
      "Epoch  93 Batch   23/49   train_loss = 0.290\n",
      "Epoch  93 Batch   33/49   train_loss = 0.343\n",
      "Epoch  93 Batch   43/49   train_loss = 0.325\n",
      "Epoch  94 Batch    4/49   train_loss = 0.324\n",
      "Epoch  94 Batch   14/49   train_loss = 0.316\n",
      "Epoch  94 Batch   24/49   train_loss = 0.368\n",
      "Epoch  94 Batch   34/49   train_loss = 0.321\n",
      "Epoch  94 Batch   44/49   train_loss = 0.302\n",
      "Epoch  95 Batch    5/49   train_loss = 0.342\n",
      "Epoch  95 Batch   15/49   train_loss = 0.353\n",
      "Epoch  95 Batch   25/49   train_loss = 0.338\n",
      "Epoch  95 Batch   35/49   train_loss = 0.353\n",
      "Epoch  95 Batch   45/49   train_loss = 0.329\n",
      "Epoch  96 Batch    6/49   train_loss = 0.331\n",
      "Epoch  96 Batch   16/49   train_loss = 0.313\n",
      "Epoch  96 Batch   26/49   train_loss = 0.353\n",
      "Epoch  96 Batch   36/49   train_loss = 0.335\n",
      "Epoch  96 Batch   46/49   train_loss = 0.301\n",
      "Epoch  97 Batch    7/49   train_loss = 0.335\n",
      "Epoch  97 Batch   17/49   train_loss = 0.329\n",
      "Epoch  97 Batch   27/49   train_loss = 0.333\n",
      "Epoch  97 Batch   37/49   train_loss = 0.347\n",
      "Epoch  97 Batch   47/49   train_loss = 0.323\n",
      "Epoch  98 Batch    8/49   train_loss = 0.309\n",
      "Epoch  98 Batch   18/49   train_loss = 0.349\n",
      "Epoch  98 Batch   28/49   train_loss = 0.305\n",
      "Epoch  98 Batch   38/49   train_loss = 0.324\n",
      "Epoch  98 Batch   48/49   train_loss = 0.334\n",
      "Epoch  99 Batch    9/49   train_loss = 0.294\n",
      "Epoch  99 Batch   19/49   train_loss = 0.300\n",
      "Epoch  99 Batch   29/49   train_loss = 0.351\n",
      "Epoch  99 Batch   39/49   train_loss = 0.307\n",
      "Epoch 100 Batch    0/49   train_loss = 0.353\n",
      "Epoch 100 Batch   10/49   train_loss = 0.327\n",
      "Epoch 100 Batch   20/49   train_loss = 0.335\n",
      "Epoch 100 Batch   30/49   train_loss = 0.350\n",
      "Epoch 100 Batch   40/49   train_loss = 0.342\n",
      "Epoch 101 Batch    1/49   train_loss = 0.338\n",
      "Epoch 101 Batch   11/49   train_loss = 0.365\n",
      "Epoch 101 Batch   21/49   train_loss = 0.328\n",
      "Epoch 101 Batch   31/49   train_loss = 0.364\n",
      "Epoch 101 Batch   41/49   train_loss = 0.323\n",
      "Epoch 102 Batch    2/49   train_loss = 0.352\n",
      "Epoch 102 Batch   12/49   train_loss = 0.317\n",
      "Epoch 102 Batch   22/49   train_loss = 0.322\n",
      "Epoch 102 Batch   32/49   train_loss = 0.353\n",
      "Epoch 102 Batch   42/49   train_loss = 0.308\n",
      "Epoch 103 Batch    3/49   train_loss = 0.319\n",
      "Epoch 103 Batch   13/49   train_loss = 0.324\n",
      "Epoch 103 Batch   23/49   train_loss = 0.286\n",
      "Epoch 103 Batch   33/49   train_loss = 0.336\n",
      "Epoch 103 Batch   43/49   train_loss = 0.324\n",
      "Epoch 104 Batch    4/49   train_loss = 0.320\n",
      "Epoch 104 Batch   14/49   train_loss = 0.312\n",
      "Epoch 104 Batch   24/49   train_loss = 0.361\n",
      "Epoch 104 Batch   34/49   train_loss = 0.315\n",
      "Epoch 104 Batch   44/49   train_loss = 0.297\n",
      "Epoch 105 Batch    5/49   train_loss = 0.337\n",
      "Epoch 105 Batch   15/49   train_loss = 0.347\n",
      "Epoch 105 Batch   25/49   train_loss = 0.332\n",
      "Epoch 105 Batch   35/49   train_loss = 0.348\n",
      "Epoch 105 Batch   45/49   train_loss = 0.322\n",
      "Epoch 106 Batch    6/49   train_loss = 0.325\n",
      "Epoch 106 Batch   16/49   train_loss = 0.306\n",
      "Epoch 106 Batch   26/49   train_loss = 0.347\n",
      "Epoch 106 Batch   36/49   train_loss = 0.331\n",
      "Epoch 106 Batch   46/49   train_loss = 0.298\n",
      "Epoch 107 Batch    7/49   train_loss = 0.329\n",
      "Epoch 107 Batch   17/49   train_loss = 0.325\n",
      "Epoch 107 Batch   27/49   train_loss = 0.327\n",
      "Epoch 107 Batch   37/49   train_loss = 0.342\n",
      "Epoch 107 Batch   47/49   train_loss = 0.315\n",
      "Epoch 108 Batch    8/49   train_loss = 0.304\n",
      "Epoch 108 Batch   18/49   train_loss = 0.343\n",
      "Epoch 108 Batch   28/49   train_loss = 0.300\n",
      "Epoch 108 Batch   38/49   train_loss = 0.322\n",
      "Epoch 108 Batch   48/49   train_loss = 0.328\n",
      "Epoch 109 Batch    9/49   train_loss = 0.289\n",
      "Epoch 109 Batch   19/49   train_loss = 0.296\n",
      "Epoch 109 Batch   29/49   train_loss = 0.348\n",
      "Epoch 109 Batch   39/49   train_loss = 0.304\n",
      "Epoch 110 Batch    0/49   train_loss = 0.347\n",
      "Epoch 110 Batch   10/49   train_loss = 0.325\n",
      "Epoch 110 Batch   20/49   train_loss = 0.330\n",
      "Epoch 110 Batch   30/49   train_loss = 0.343\n",
      "Epoch 110 Batch   40/49   train_loss = 0.333\n",
      "Epoch 111 Batch    1/49   train_loss = 0.335\n",
      "Epoch 111 Batch   11/49   train_loss = 0.361\n",
      "Epoch 111 Batch   21/49   train_loss = 0.323\n",
      "Epoch 111 Batch   31/49   train_loss = 0.360\n",
      "Epoch 111 Batch   41/49   train_loss = 0.320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112 Batch    2/49   train_loss = 0.348\n",
      "Epoch 112 Batch   12/49   train_loss = 0.311\n",
      "Epoch 112 Batch   22/49   train_loss = 0.319\n",
      "Epoch 112 Batch   32/49   train_loss = 0.347\n",
      "Epoch 112 Batch   42/49   train_loss = 0.303\n",
      "Epoch 113 Batch    3/49   train_loss = 0.314\n",
      "Epoch 113 Batch   13/49   train_loss = 0.321\n",
      "Epoch 113 Batch   23/49   train_loss = 0.282\n",
      "Epoch 113 Batch   33/49   train_loss = 0.334\n",
      "Epoch 113 Batch   43/49   train_loss = 0.319\n",
      "Epoch 114 Batch    4/49   train_loss = 0.316\n",
      "Epoch 114 Batch   14/49   train_loss = 0.308\n",
      "Epoch 114 Batch   24/49   train_loss = 0.358\n",
      "Epoch 114 Batch   34/49   train_loss = 0.313\n",
      "Epoch 114 Batch   44/49   train_loss = 0.293\n",
      "Epoch 115 Batch    5/49   train_loss = 0.333\n",
      "Epoch 115 Batch   15/49   train_loss = 0.344\n",
      "Epoch 115 Batch   25/49   train_loss = 0.329\n",
      "Epoch 115 Batch   35/49   train_loss = 0.344\n",
      "Epoch 115 Batch   45/49   train_loss = 0.318\n",
      "Epoch 116 Batch    6/49   train_loss = 0.324\n",
      "Epoch 116 Batch   16/49   train_loss = 0.304\n",
      "Epoch 116 Batch   26/49   train_loss = 0.343\n",
      "Epoch 116 Batch   36/49   train_loss = 0.329\n",
      "Epoch 116 Batch   46/49   train_loss = 0.294\n",
      "Epoch 117 Batch    7/49   train_loss = 0.326\n",
      "Epoch 117 Batch   17/49   train_loss = 0.321\n",
      "Epoch 117 Batch   27/49   train_loss = 0.325\n",
      "Epoch 117 Batch   37/49   train_loss = 0.339\n",
      "Epoch 117 Batch   47/49   train_loss = 0.313\n",
      "Epoch 118 Batch    8/49   train_loss = 0.301\n",
      "Epoch 118 Batch   18/49   train_loss = 0.340\n",
      "Epoch 118 Batch   28/49   train_loss = 0.295\n",
      "Epoch 118 Batch   38/49   train_loss = 0.317\n",
      "Epoch 118 Batch   48/49   train_loss = 0.324\n",
      "Epoch 119 Batch    9/49   train_loss = 0.285\n",
      "Epoch 119 Batch   19/49   train_loss = 0.295\n",
      "Epoch 119 Batch   29/49   train_loss = 0.345\n",
      "Epoch 119 Batch   39/49   train_loss = 0.301\n",
      "Epoch 120 Batch    0/49   train_loss = 0.345\n",
      "Epoch 120 Batch   10/49   train_loss = 0.322\n",
      "Epoch 120 Batch   20/49   train_loss = 0.329\n",
      "Epoch 120 Batch   30/49   train_loss = 0.340\n",
      "Epoch 120 Batch   40/49   train_loss = 0.332\n",
      "Epoch 121 Batch    1/49   train_loss = 0.334\n",
      "Epoch 121 Batch   11/49   train_loss = 0.360\n",
      "Epoch 121 Batch   21/49   train_loss = 0.319\n",
      "Epoch 121 Batch   31/49   train_loss = 0.356\n",
      "Epoch 121 Batch   41/49   train_loss = 0.317\n",
      "Epoch 122 Batch    2/49   train_loss = 0.344\n",
      "Epoch 122 Batch   12/49   train_loss = 0.308\n",
      "Epoch 122 Batch   22/49   train_loss = 0.315\n",
      "Epoch 122 Batch   32/49   train_loss = 0.345\n",
      "Epoch 122 Batch   42/49   train_loss = 0.301\n",
      "Epoch 123 Batch    3/49   train_loss = 0.312\n",
      "Epoch 123 Batch   13/49   train_loss = 0.317\n",
      "Epoch 123 Batch   23/49   train_loss = 0.280\n",
      "Epoch 123 Batch   33/49   train_loss = 0.331\n",
      "Epoch 123 Batch   43/49   train_loss = 0.316\n",
      "Epoch 124 Batch    4/49   train_loss = 0.314\n",
      "Epoch 124 Batch   14/49   train_loss = 0.307\n",
      "Epoch 124 Batch   24/49   train_loss = 0.356\n",
      "Epoch 124 Batch   34/49   train_loss = 0.310\n",
      "Epoch 124 Batch   44/49   train_loss = 0.292\n",
      "Epoch 125 Batch    5/49   train_loss = 0.330\n",
      "Epoch 125 Batch   15/49   train_loss = 0.342\n",
      "Epoch 125 Batch   25/49   train_loss = 0.329\n",
      "Epoch 125 Batch   35/49   train_loss = 0.343\n",
      "Epoch 125 Batch   45/49   train_loss = 0.318\n",
      "Epoch 126 Batch    6/49   train_loss = 0.321\n",
      "Epoch 126 Batch   16/49   train_loss = 0.303\n",
      "Epoch 126 Batch   26/49   train_loss = 0.341\n",
      "Epoch 126 Batch   36/49   train_loss = 0.324\n",
      "Epoch 126 Batch   46/49   train_loss = 0.292\n",
      "Epoch 127 Batch    7/49   train_loss = 0.324\n",
      "Epoch 127 Batch   17/49   train_loss = 0.319\n",
      "Epoch 127 Batch   27/49   train_loss = 0.323\n",
      "Epoch 127 Batch   37/49   train_loss = 0.337\n",
      "Epoch 127 Batch   47/49   train_loss = 0.313\n",
      "Epoch 128 Batch    8/49   train_loss = 0.300\n",
      "Epoch 128 Batch   18/49   train_loss = 0.339\n",
      "Epoch 128 Batch   28/49   train_loss = 0.295\n",
      "Epoch 128 Batch   38/49   train_loss = 0.317\n",
      "Epoch 128 Batch   48/49   train_loss = 0.325\n",
      "Epoch 129 Batch    9/49   train_loss = 0.282\n",
      "Epoch 129 Batch   19/49   train_loss = 0.294\n",
      "Epoch 129 Batch   29/49   train_loss = 0.343\n",
      "Epoch 129 Batch   39/49   train_loss = 0.300\n",
      "Epoch 130 Batch    0/49   train_loss = 0.342\n",
      "Epoch 130 Batch   10/49   train_loss = 0.321\n",
      "Epoch 130 Batch   20/49   train_loss = 0.327\n",
      "Epoch 130 Batch   30/49   train_loss = 0.340\n",
      "Epoch 130 Batch   40/49   train_loss = 0.329\n",
      "Epoch 131 Batch    1/49   train_loss = 0.331\n",
      "Epoch 131 Batch   11/49   train_loss = 0.357\n",
      "Epoch 131 Batch   21/49   train_loss = 0.316\n",
      "Epoch 131 Batch   31/49   train_loss = 0.355\n",
      "Epoch 131 Batch   41/49   train_loss = 0.315\n",
      "Epoch 132 Batch    2/49   train_loss = 0.344\n",
      "Epoch 132 Batch   12/49   train_loss = 0.307\n",
      "Epoch 132 Batch   22/49   train_loss = 0.316\n",
      "Epoch 132 Batch   32/49   train_loss = 0.346\n",
      "Epoch 132 Batch   42/49   train_loss = 0.298\n",
      "Epoch 133 Batch    3/49   train_loss = 0.313\n",
      "Epoch 133 Batch   13/49   train_loss = 0.315\n",
      "Epoch 133 Batch   23/49   train_loss = 0.280\n",
      "Epoch 133 Batch   33/49   train_loss = 0.330\n",
      "Epoch 133 Batch   43/49   train_loss = 0.317\n",
      "Epoch 134 Batch    4/49   train_loss = 0.313\n",
      "Epoch 134 Batch   14/49   train_loss = 0.305\n",
      "Epoch 134 Batch   24/49   train_loss = 0.353\n",
      "Epoch 134 Batch   34/49   train_loss = 0.309\n",
      "Epoch 134 Batch   44/49   train_loss = 0.291\n",
      "Epoch 135 Batch    5/49   train_loss = 0.328\n",
      "Epoch 135 Batch   15/49   train_loss = 0.342\n",
      "Epoch 135 Batch   25/49   train_loss = 0.325\n",
      "Epoch 135 Batch   35/49   train_loss = 0.343\n",
      "Epoch 135 Batch   45/49   train_loss = 0.316\n",
      "Epoch 136 Batch    6/49   train_loss = 0.319\n",
      "Epoch 136 Batch   16/49   train_loss = 0.302\n",
      "Epoch 136 Batch   26/49   train_loss = 0.340\n",
      "Epoch 136 Batch   36/49   train_loss = 0.324\n",
      "Epoch 136 Batch   46/49   train_loss = 0.291\n",
      "Epoch 137 Batch    7/49   train_loss = 0.324\n",
      "Epoch 137 Batch   17/49   train_loss = 0.319\n",
      "Epoch 137 Batch   27/49   train_loss = 0.322\n",
      "Epoch 137 Batch   37/49   train_loss = 0.336\n",
      "Epoch 137 Batch   47/49   train_loss = 0.311\n",
      "Epoch 138 Batch    8/49   train_loss = 0.298\n",
      "Epoch 138 Batch   18/49   train_loss = 0.338\n",
      "Epoch 138 Batch   28/49   train_loss = 0.294\n",
      "Epoch 138 Batch   38/49   train_loss = 0.315\n",
      "Epoch 138 Batch   48/49   train_loss = 0.322\n",
      "Epoch 139 Batch    9/49   train_loss = 0.281\n",
      "Epoch 139 Batch   19/49   train_loss = 0.293\n",
      "Epoch 139 Batch   29/49   train_loss = 0.342\n",
      "Epoch 139 Batch   39/49   train_loss = 0.299\n",
      "Epoch 140 Batch    0/49   train_loss = 0.340\n",
      "Epoch 140 Batch   10/49   train_loss = 0.319\n",
      "Epoch 140 Batch   20/49   train_loss = 0.326\n",
      "Epoch 140 Batch   30/49   train_loss = 0.337\n",
      "Epoch 140 Batch   40/49   train_loss = 0.329\n",
      "Epoch 141 Batch    1/49   train_loss = 0.330\n",
      "Epoch 141 Batch   11/49   train_loss = 0.355\n",
      "Epoch 141 Batch   21/49   train_loss = 0.318\n",
      "Epoch 141 Batch   31/49   train_loss = 0.353\n",
      "Epoch 141 Batch   41/49   train_loss = 0.314\n",
      "Epoch 142 Batch    2/49   train_loss = 0.342\n",
      "Epoch 142 Batch   12/49   train_loss = 0.306\n",
      "Epoch 142 Batch   22/49   train_loss = 0.313\n",
      "Epoch 142 Batch   32/49   train_loss = 0.343\n",
      "Epoch 142 Batch   42/49   train_loss = 0.297\n",
      "Epoch 143 Batch    3/49   train_loss = 0.310\n",
      "Epoch 143 Batch   13/49   train_loss = 0.314\n",
      "Epoch 143 Batch   23/49   train_loss = 0.277\n",
      "Epoch 143 Batch   33/49   train_loss = 0.328\n",
      "Epoch 143 Batch   43/49   train_loss = 0.314\n",
      "Epoch 144 Batch    4/49   train_loss = 0.311\n",
      "Epoch 144 Batch   14/49   train_loss = 0.303\n",
      "Epoch 144 Batch   24/49   train_loss = 0.353\n",
      "Epoch 144 Batch   34/49   train_loss = 0.309\n",
      "Epoch 144 Batch   44/49   train_loss = 0.289\n",
      "Epoch 145 Batch    5/49   train_loss = 0.329\n",
      "Epoch 145 Batch   15/49   train_loss = 0.340\n",
      "Epoch 145 Batch   25/49   train_loss = 0.324\n",
      "Epoch 145 Batch   35/49   train_loss = 0.339\n",
      "Epoch 145 Batch   45/49   train_loss = 0.315\n",
      "Epoch 146 Batch    6/49   train_loss = 0.320\n",
      "Epoch 146 Batch   16/49   train_loss = 0.300\n",
      "Epoch 146 Batch   26/49   train_loss = 0.338\n",
      "Epoch 146 Batch   36/49   train_loss = 0.322\n",
      "Epoch 146 Batch   46/49   train_loss = 0.289\n",
      "Epoch 147 Batch    7/49   train_loss = 0.322\n",
      "Epoch 147 Batch   17/49   train_loss = 0.317\n",
      "Epoch 147 Batch   27/49   train_loss = 0.321\n",
      "Epoch 147 Batch   37/49   train_loss = 0.335\n",
      "Epoch 147 Batch   47/49   train_loss = 0.309\n",
      "Epoch 148 Batch    8/49   train_loss = 0.297\n",
      "Epoch 148 Batch   18/49   train_loss = 0.337\n",
      "Epoch 148 Batch   28/49   train_loss = 0.292\n",
      "Epoch 148 Batch   38/49   train_loss = 0.314\n",
      "Epoch 148 Batch   48/49   train_loss = 0.321\n",
      "Epoch 149 Batch    9/49   train_loss = 0.279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149 Batch   19/49   train_loss = 0.289\n",
      "Epoch 149 Batch   29/49   train_loss = 0.339\n",
      "Epoch 149 Batch   39/49   train_loss = 0.298\n",
      "Epoch 150 Batch    0/49   train_loss = 0.340\n",
      "Epoch 150 Batch   10/49   train_loss = 0.318\n",
      "Epoch 150 Batch   20/49   train_loss = 0.325\n",
      "Epoch 150 Batch   30/49   train_loss = 0.338\n",
      "Epoch 150 Batch   40/49   train_loss = 0.327\n",
      "Epoch 151 Batch    1/49   train_loss = 0.328\n",
      "Epoch 151 Batch   11/49   train_loss = 0.354\n",
      "Epoch 151 Batch   21/49   train_loss = 0.314\n",
      "Epoch 151 Batch   31/49   train_loss = 0.352\n",
      "Epoch 151 Batch   41/49   train_loss = 0.313\n",
      "Epoch 152 Batch    2/49   train_loss = 0.341\n",
      "Epoch 152 Batch   12/49   train_loss = 0.305\n",
      "Epoch 152 Batch   22/49   train_loss = 0.312\n",
      "Epoch 152 Batch   32/49   train_loss = 0.343\n",
      "Epoch 152 Batch   42/49   train_loss = 0.296\n",
      "Epoch 153 Batch    3/49   train_loss = 0.310\n",
      "Epoch 153 Batch   13/49   train_loss = 0.312\n",
      "Epoch 153 Batch   23/49   train_loss = 0.277\n",
      "Epoch 153 Batch   33/49   train_loss = 0.326\n",
      "Epoch 153 Batch   43/49   train_loss = 0.313\n",
      "Epoch 154 Batch    4/49   train_loss = 0.309\n",
      "Epoch 154 Batch   14/49   train_loss = 0.301\n",
      "Epoch 154 Batch   24/49   train_loss = 0.350\n",
      "Epoch 154 Batch   34/49   train_loss = 0.306\n",
      "Epoch 154 Batch   44/49   train_loss = 0.288\n",
      "Epoch 155 Batch    5/49   train_loss = 0.325\n",
      "Epoch 155 Batch   15/49   train_loss = 0.338\n",
      "Epoch 155 Batch   25/49   train_loss = 0.323\n",
      "Epoch 155 Batch   35/49   train_loss = 0.338\n",
      "Epoch 155 Batch   45/49   train_loss = 0.313\n",
      "Epoch 156 Batch    6/49   train_loss = 0.317\n",
      "Epoch 156 Batch   16/49   train_loss = 0.299\n",
      "Epoch 156 Batch   26/49   train_loss = 0.336\n",
      "Epoch 156 Batch   36/49   train_loss = 0.321\n",
      "Epoch 156 Batch   46/49   train_loss = 0.288\n",
      "Epoch 157 Batch    7/49   train_loss = 0.320\n",
      "Epoch 157 Batch   17/49   train_loss = 0.315\n",
      "Epoch 157 Batch   27/49   train_loss = 0.318\n",
      "Epoch 157 Batch   37/49   train_loss = 0.333\n",
      "Epoch 157 Batch   47/49   train_loss = 0.307\n",
      "Epoch 158 Batch    8/49   train_loss = 0.295\n",
      "Epoch 158 Batch   18/49   train_loss = 0.335\n",
      "Epoch 158 Batch   28/49   train_loss = 0.290\n",
      "Epoch 158 Batch   38/49   train_loss = 0.312\n",
      "Epoch 158 Batch   48/49   train_loss = 0.319\n",
      "Epoch 159 Batch    9/49   train_loss = 0.278\n",
      "Epoch 159 Batch   19/49   train_loss = 0.289\n",
      "Epoch 159 Batch   29/49   train_loss = 0.338\n",
      "Epoch 159 Batch   39/49   train_loss = 0.296\n",
      "Epoch 160 Batch    0/49   train_loss = 0.337\n",
      "Epoch 160 Batch   10/49   train_loss = 0.316\n",
      "Epoch 160 Batch   20/49   train_loss = 0.322\n",
      "Epoch 160 Batch   30/49   train_loss = 0.335\n",
      "Epoch 160 Batch   40/49   train_loss = 0.325\n",
      "Epoch 161 Batch    1/49   train_loss = 0.327\n",
      "Epoch 161 Batch   11/49   train_loss = 0.352\n",
      "Epoch 161 Batch   21/49   train_loss = 0.312\n",
      "Epoch 161 Batch   31/49   train_loss = 0.349\n",
      "Epoch 161 Batch   41/49   train_loss = 0.310\n",
      "Epoch 162 Batch    2/49   train_loss = 0.339\n",
      "Epoch 162 Batch   12/49   train_loss = 0.302\n",
      "Epoch 162 Batch   22/49   train_loss = 0.311\n",
      "Epoch 162 Batch   32/49   train_loss = 0.341\n",
      "Epoch 162 Batch   42/49   train_loss = 0.294\n",
      "Epoch 163 Batch    3/49   train_loss = 0.307\n",
      "Epoch 163 Batch   13/49   train_loss = 0.311\n",
      "Epoch 163 Batch   23/49   train_loss = 0.276\n",
      "Epoch 163 Batch   33/49   train_loss = 0.326\n",
      "Epoch 163 Batch   43/49   train_loss = 0.311\n",
      "Epoch 164 Batch    4/49   train_loss = 0.307\n",
      "Epoch 164 Batch   14/49   train_loss = 0.300\n",
      "Epoch 164 Batch   24/49   train_loss = 0.349\n",
      "Epoch 164 Batch   34/49   train_loss = 0.306\n",
      "Epoch 164 Batch   44/49   train_loss = 0.287\n",
      "Epoch 165 Batch    5/49   train_loss = 0.324\n",
      "Epoch 165 Batch   15/49   train_loss = 0.337\n",
      "Epoch 165 Batch   25/49   train_loss = 0.321\n",
      "Epoch 165 Batch   35/49   train_loss = 0.336\n",
      "Epoch 165 Batch   45/49   train_loss = 0.312\n",
      "Epoch 166 Batch    6/49   train_loss = 0.316\n",
      "Epoch 166 Batch   16/49   train_loss = 0.298\n",
      "Epoch 166 Batch   26/49   train_loss = 0.336\n",
      "Epoch 166 Batch   36/49   train_loss = 0.318\n",
      "Epoch 166 Batch   46/49   train_loss = 0.286\n",
      "Epoch 167 Batch    7/49   train_loss = 0.320\n",
      "Epoch 167 Batch   17/49   train_loss = 0.314\n",
      "Epoch 167 Batch   27/49   train_loss = 0.318\n",
      "Epoch 167 Batch   37/49   train_loss = 0.332\n",
      "Epoch 167 Batch   47/49   train_loss = 0.306\n",
      "Epoch 168 Batch    8/49   train_loss = 0.294\n",
      "Epoch 168 Batch   18/49   train_loss = 0.333\n",
      "Epoch 168 Batch   28/49   train_loss = 0.290\n",
      "Epoch 168 Batch   38/49   train_loss = 0.312\n",
      "Epoch 168 Batch   48/49   train_loss = 0.318\n",
      "Epoch 169 Batch    9/49   train_loss = 0.276\n",
      "Epoch 169 Batch   19/49   train_loss = 0.288\n",
      "Epoch 169 Batch   29/49   train_loss = 0.337\n",
      "Epoch 169 Batch   39/49   train_loss = 0.295\n",
      "Epoch 170 Batch    0/49   train_loss = 0.337\n",
      "Epoch 170 Batch   10/49   train_loss = 0.315\n",
      "Epoch 170 Batch   20/49   train_loss = 0.321\n",
      "Epoch 170 Batch   30/49   train_loss = 0.335\n",
      "Epoch 170 Batch   40/49   train_loss = 0.324\n",
      "Epoch 171 Batch    1/49   train_loss = 0.326\n",
      "Epoch 171 Batch   11/49   train_loss = 0.350\n",
      "Epoch 171 Batch   21/49   train_loss = 0.312\n",
      "Epoch 171 Batch   31/49   train_loss = 0.348\n",
      "Epoch 171 Batch   41/49   train_loss = 0.309\n",
      "Epoch 172 Batch    2/49   train_loss = 0.338\n",
      "Epoch 172 Batch   12/49   train_loss = 0.302\n",
      "Epoch 172 Batch   22/49   train_loss = 0.310\n",
      "Epoch 172 Batch   32/49   train_loss = 0.341\n",
      "Epoch 172 Batch   42/49   train_loss = 0.293\n",
      "Epoch 173 Batch    3/49   train_loss = 0.307\n",
      "Epoch 173 Batch   13/49   train_loss = 0.309\n",
      "Epoch 173 Batch   23/49   train_loss = 0.275\n",
      "Epoch 173 Batch   33/49   train_loss = 0.324\n",
      "Epoch 173 Batch   43/49   train_loss = 0.311\n",
      "Epoch 174 Batch    4/49   train_loss = 0.307\n",
      "Epoch 174 Batch   14/49   train_loss = 0.298\n",
      "Epoch 174 Batch   24/49   train_loss = 0.348\n",
      "Epoch 174 Batch   34/49   train_loss = 0.305\n",
      "Epoch 174 Batch   44/49   train_loss = 0.285\n",
      "Epoch 175 Batch    5/49   train_loss = 0.324\n",
      "Epoch 175 Batch   15/49   train_loss = 0.336\n",
      "Epoch 175 Batch   25/49   train_loss = 0.320\n",
      "Epoch 175 Batch   35/49   train_loss = 0.335\n",
      "Epoch 175 Batch   45/49   train_loss = 0.310\n",
      "Epoch 176 Batch    6/49   train_loss = 0.315\n",
      "Epoch 176 Batch   16/49   train_loss = 0.297\n",
      "Epoch 176 Batch   26/49   train_loss = 0.334\n",
      "Epoch 176 Batch   36/49   train_loss = 0.318\n",
      "Epoch 176 Batch   46/49   train_loss = 0.286\n",
      "Epoch 177 Batch    7/49   train_loss = 0.318\n",
      "Epoch 177 Batch   17/49   train_loss = 0.313\n",
      "Epoch 177 Batch   27/49   train_loss = 0.317\n",
      "Epoch 177 Batch   37/49   train_loss = 0.330\n",
      "Epoch 177 Batch   47/49   train_loss = 0.305\n",
      "Epoch 178 Batch    8/49   train_loss = 0.292\n",
      "Epoch 178 Batch   18/49   train_loss = 0.333\n",
      "Epoch 178 Batch   28/49   train_loss = 0.288\n",
      "Epoch 178 Batch   38/49   train_loss = 0.311\n",
      "Epoch 178 Batch   48/49   train_loss = 0.316\n",
      "Epoch 179 Batch    9/49   train_loss = 0.275\n",
      "Epoch 179 Batch   19/49   train_loss = 0.286\n",
      "Epoch 179 Batch   29/49   train_loss = 0.336\n",
      "Epoch 179 Batch   39/49   train_loss = 0.295\n",
      "Epoch 180 Batch    0/49   train_loss = 0.335\n",
      "Epoch 180 Batch   10/49   train_loss = 0.314\n",
      "Epoch 180 Batch   20/49   train_loss = 0.321\n",
      "Epoch 180 Batch   30/49   train_loss = 0.333\n",
      "Epoch 180 Batch   40/49   train_loss = 0.323\n",
      "Epoch 181 Batch    1/49   train_loss = 0.324\n",
      "Epoch 181 Batch   11/49   train_loss = 0.349\n",
      "Epoch 181 Batch   21/49   train_loss = 0.310\n",
      "Epoch 181 Batch   31/49   train_loss = 0.348\n",
      "Epoch 181 Batch   41/49   train_loss = 0.308\n",
      "Epoch 182 Batch    2/49   train_loss = 0.337\n",
      "Epoch 182 Batch   12/49   train_loss = 0.301\n",
      "Epoch 182 Batch   22/49   train_loss = 0.309\n",
      "Epoch 182 Batch   32/49   train_loss = 0.339\n",
      "Epoch 182 Batch   42/49   train_loss = 0.292\n",
      "Epoch 183 Batch    3/49   train_loss = 0.306\n",
      "Epoch 183 Batch   13/49   train_loss = 0.309\n",
      "Epoch 183 Batch   23/49   train_loss = 0.274\n",
      "Epoch 183 Batch   33/49   train_loss = 0.323\n",
      "Epoch 183 Batch   43/49   train_loss = 0.310\n",
      "Epoch 184 Batch    4/49   train_loss = 0.306\n",
      "Epoch 184 Batch   14/49   train_loss = 0.297\n",
      "Epoch 184 Batch   24/49   train_loss = 0.347\n",
      "Epoch 184 Batch   34/49   train_loss = 0.303\n",
      "Epoch 184 Batch   44/49   train_loss = 0.285\n",
      "Epoch 185 Batch    5/49   train_loss = 0.322\n",
      "Epoch 185 Batch   15/49   train_loss = 0.335\n",
      "Epoch 185 Batch   25/49   train_loss = 0.319\n",
      "Epoch 185 Batch   35/49   train_loss = 0.334\n",
      "Epoch 185 Batch   45/49   train_loss = 0.310\n",
      "Epoch 186 Batch    6/49   train_loss = 0.314\n",
      "Epoch 186 Batch   16/49   train_loss = 0.296\n",
      "Epoch 186 Batch   26/49   train_loss = 0.334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186 Batch   36/49   train_loss = 0.317\n",
      "Epoch 186 Batch   46/49   train_loss = 0.285\n",
      "Epoch 187 Batch    7/49   train_loss = 0.317\n",
      "Epoch 187 Batch   17/49   train_loss = 0.312\n",
      "Epoch 187 Batch   27/49   train_loss = 0.316\n",
      "Epoch 187 Batch   37/49   train_loss = 0.330\n",
      "Epoch 187 Batch   47/49   train_loss = 0.304\n",
      "Epoch 188 Batch    8/49   train_loss = 0.291\n",
      "Epoch 188 Batch   18/49   train_loss = 0.331\n",
      "Epoch 188 Batch   28/49   train_loss = 0.288\n",
      "Epoch 188 Batch   38/49   train_loss = 0.310\n",
      "Epoch 188 Batch   48/49   train_loss = 0.316\n",
      "Epoch 189 Batch    9/49   train_loss = 0.274\n",
      "Epoch 189 Batch   19/49   train_loss = 0.286\n",
      "Epoch 189 Batch   29/49   train_loss = 0.335\n",
      "Epoch 189 Batch   39/49   train_loss = 0.294\n",
      "Epoch 190 Batch    0/49   train_loss = 0.335\n",
      "Epoch 190 Batch   10/49   train_loss = 0.313\n",
      "Epoch 190 Batch   20/49   train_loss = 0.319\n",
      "Epoch 190 Batch   30/49   train_loss = 0.333\n",
      "Epoch 190 Batch   40/49   train_loss = 0.322\n",
      "Epoch 191 Batch    1/49   train_loss = 0.324\n",
      "Epoch 191 Batch   11/49   train_loss = 0.348\n",
      "Epoch 191 Batch   21/49   train_loss = 0.310\n",
      "Epoch 191 Batch   31/49   train_loss = 0.346\n",
      "Epoch 191 Batch   41/49   train_loss = 0.307\n",
      "Epoch 192 Batch    2/49   train_loss = 0.336\n",
      "Epoch 192 Batch   12/49   train_loss = 0.300\n",
      "Epoch 192 Batch   22/49   train_loss = 0.308\n",
      "Epoch 192 Batch   32/49   train_loss = 0.339\n",
      "Epoch 192 Batch   42/49   train_loss = 0.291\n",
      "Epoch 193 Batch    3/49   train_loss = 0.305\n",
      "Epoch 193 Batch   13/49   train_loss = 0.308\n",
      "Epoch 193 Batch   23/49   train_loss = 0.273\n",
      "Epoch 193 Batch   33/49   train_loss = 0.322\n",
      "Epoch 193 Batch   43/49   train_loss = 0.309\n",
      "Epoch 194 Batch    4/49   train_loss = 0.305\n",
      "Epoch 194 Batch   14/49   train_loss = 0.297\n",
      "Epoch 194 Batch   24/49   train_loss = 0.346\n",
      "Epoch 194 Batch   34/49   train_loss = 0.303\n",
      "Epoch 194 Batch   44/49   train_loss = 0.284\n",
      "Epoch 195 Batch    5/49   train_loss = 0.322\n",
      "Epoch 195 Batch   15/49   train_loss = 0.335\n",
      "Epoch 195 Batch   25/49   train_loss = 0.318\n",
      "Epoch 195 Batch   35/49   train_loss = 0.334\n",
      "Epoch 195 Batch   45/49   train_loss = 0.309\n",
      "Epoch 196 Batch    6/49   train_loss = 0.313\n",
      "Epoch 196 Batch   16/49   train_loss = 0.295\n",
      "Epoch 196 Batch   26/49   train_loss = 0.333\n",
      "Epoch 196 Batch   36/49   train_loss = 0.316\n",
      "Epoch 196 Batch   46/49   train_loss = 0.284\n",
      "Epoch 197 Batch    7/49   train_loss = 0.317\n",
      "Epoch 197 Batch   17/49   train_loss = 0.311\n",
      "Epoch 197 Batch   27/49   train_loss = 0.316\n",
      "Epoch 197 Batch   37/49   train_loss = 0.329\n",
      "Epoch 197 Batch   47/49   train_loss = 0.303\n",
      "Epoch 198 Batch    8/49   train_loss = 0.291\n",
      "Epoch 198 Batch   18/49   train_loss = 0.331\n",
      "Epoch 198 Batch   28/49   train_loss = 0.287\n",
      "Epoch 198 Batch   38/49   train_loss = 0.309\n",
      "Epoch 198 Batch   48/49   train_loss = 0.315\n",
      "Epoch 199 Batch    9/49   train_loss = 0.274\n",
      "Epoch 199 Batch   19/49   train_loss = 0.284\n",
      "Epoch 199 Batch   29/49   train_loss = 0.334\n",
      "Epoch 199 Batch   39/49   train_loss = 0.293\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 储存参数\n",
    "储存 `seq_length` 和 `save_dir` 来生成新的电视剧剧本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Save parameters for checkpoint\n",
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现生成函数\n",
    "### 获取 Tensors\n",
    "使用 [`get_tensor_by_name()`](https://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name)函数从 `loaded_graph` 中获取 tensor。  使用下面的名称获取 tensor：\n",
    "\n",
    "- \"input:0\"\n",
    "- \"initial_state:0\"\n",
    "- \"final_state:0\"\n",
    "- \"probs:0\"\n",
    "\n",
    "返回下列元组中的 tensor `(InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input_ = loaded_graph.get_tensor_by_name(\"input:0\")\n",
    "    initial_state = loaded_graph.get_tensor_by_name(\"initial_state:0\")\n",
    "    final_state = loaded_graph.get_tensor_by_name(\"final_state:0\")\n",
    "    probs = loaded_graph.get_tensor_by_name(\"probs:0\")\n",
    "    output = (input_, initial_state, final_state, probs)\n",
    "    return output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_tensors(get_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 选择词汇\n",
    "实现 `pick_word()` 函数来使用 `probabilities` 选择下一个词汇。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # 错误代码 \n",
    "#     import heapq,random\n",
    "#     idx = [idx for idx, prob in heapq.nlargest(5,enumerate(probabilities))]\n",
    "#     pos_idx = idx[random.randint(0,4)]\n",
    "#     word = int_to_vocab[pos_idx]\n",
    "    return np.random.choice(list(int_to_vocab.values()),1,p=probabilities)[0]\n",
    "    # 这里的思路是选几个个概率最大的位置出来，然后随机选择一个位置去找相应的词是什么\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_pick_word(pick_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成电视剧剧本\n",
    "这将为你生成一个电视剧剧本。通过设置 `gen_length` 来调整你想生成的剧本长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The saved meta_graph is possibly from an older release:\n",
      "'model_variables' collection should be of type 'byte_list', but instead is of type 'node_list'.\n",
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "moe_szyslak: easy, easy there, lenny. you can always play. my kids was back, but i speak to get ya to a little.\n",
      "moe_szyslak: what happened? it's a kid who wears a\"(to film) atlanta falcons of a... might be home of people.\n",
      "moe_szyslak:(worried) how you know, i will say or this town a night.?\n",
      "renee:(hopeful) i don't know... hello maggie.\n",
      "homer_simpson: look, i made a good.\n",
      "moe_szyslak: if that's homer to scare you.\n",
      "moe_szyslak:(into phone) i think we've had a little more of it is.\n",
      "homer_simpson: i had to do. i didn't get quite?\n",
      "moe_szyslak:(sighs) is there it?\n",
      "think i knew this much would you guys dennis in your face... homer, i'm gonna let you to i'd give you guys need a beer for now this. oh, you're a beer!\n",
      "moe_szyslak:(into phone) that's a moe's tavern.\n",
      "homer_simpson: listen,\n"
     ]
    }
   ],
   "source": [
    "gen_length = 200\n",
    "# homer_simpson, moe_szyslak, or Barney_Gumble\n",
    "prime_word = 'moe_szyslak'\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word + ':']\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这个电视剧剧本是无意义的\n",
    "如果这个电视剧剧本毫无意义，那也没有关系。我们的训练文本不到一兆字节。为了获得更好的结果，你需要使用更小的词汇范围或是更多数据。幸运的是，我们的确拥有更多数据！在本项目开始之初我们也曾提过，这是[另一个数据集](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data)的子集。我们并没有让你基于所有数据进行训练，因为这将耗费大量时间。然而，你可以随意使用这些数据训练你的神经网络。当然，是在完成本项目之后。\n",
    "# 提交项目\n",
    "在提交项目时，请确保你在保存 notebook 前运行了所有的单元格代码。请将 notebook 文件保存为 \"dlnd_tv_script_generation.ipynb\"，并将它作为 HTML 文件保存在 \"File\" -> \"Download as\" 中。请将 \"helper.py\" 和 \"problem_unittests.py\" 文件一并提交。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
